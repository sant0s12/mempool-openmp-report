\chapter{Implementation}
\label{ch:implementation}

\section{General Architecture}
\label{sec:general-architecture}

The OpenMP runtime library presented in this report is implemented in C++ and is structured into
the following major components:

\begin{itemize}
	\item KMP entrypoints.
	\item C++ classes for each relevant concept (e.g., threads, teams, barriers, etc.).
	\item Supporting code that bridges the gap between higher level concepts and the underlying
	      MemPool architecture.
\end{itemize}

Each of them will be described in more detail in the following sections.

\section{KMP Entrypoints}
\label{sec:kmp-entrypoints}

The KMP entrypoints are the functions that are called at runtime when running an OpenMP program.
During the compilation process, the compiler converts OpenMP directives into calls to these
functions as defined by the internal \gls{api} between the LLVM compiler and the runtime library. An
LLVM OpenMP runtime reference document is available at~\cite{kmpref}, however it is quite out of date
and not very helpful in documenting the expected behavior of the runtime library. For this reason,
using the source code of the default runtime
library\footnote{\url{https://github.com/llvm/llvm-project/tree/main/openmp}}, as well as smaller
third-party implementations\footnote{\url{https://github.com/parallel-runtimes/lomp}} can be quite
helpful.

In the following, we will go trough each of the implemented entrypoints grouped by the OpenMP
construct or clause that requires them.

\subsection{Parallel Construct}
\label{subsec:parallel-construct}

\subsubsection{\texttt{\_\_kmpc\_fork\_call}}
\label{subsubsec:kmpc-fork-call}

\paragraph{Description} This function is called by the master thread of the current team (the
default team contains all cores and the core with ID 0 runs the master thread). It is responsible
for assigning the appropriate number of threads to the team and waking them up, as well as setting
up the task that they will run.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{argc}: Number of arguments passed to the microtask function.
	\item \texttt{microtask}: Function pointer to the task to be run by each thread in the team.
	\item \texttt{...}: Variable number of arguments to be passed to the microtask function.
\end{itemize}

\paragraph{Implementation} First, it creates a \texttt{kmp::Task} (\todo{ref}) object with the
microtask, arguments casted to a void pointer array, and the number of such arguments. Then, it
obtains the object representing the current thread using \texttt{kmp::runtime::getCurrentThread()}
(\todo{ref}) and calls the \texttt{forkCall} (\todo{ref}) method on it with the task as an argument.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_fork\_call}, label={lst:fork-call},
                   escapechar=@]
void __kmpc_fork_call(ident_t *loc, kmp_int32 argc,
                      kmpc_micro microtask, ...) {
  va_list args;
  va_start(args, microtask);
  kmp::Task kmpMicrotask(microtask, reinterpret_cast<void **>(args),
                         argc);
  kmp::runtime::getCurrentThread().forkCall(kmpMicrotask);
  va_end(args);
};
\end{lstlisting}

\subsubsection{\texttt{\_\_kmpc\_push\_num\_threads}}

\paragraph{Description} This function is called when using the \texttt{num\_threads} clause in a
parallel construct. It is used to request a specific number of threads to be used in the parallel
section.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
	\item \texttt{num\_threads}: Requested number of threads.
\end{itemize}

\paragraph{Implementation} This function essentially just calls the \texttt{requestNumThreads}
method (\todo{ref}) on the object representing the current thread obtained by calling
\texttt{kmp::runtime::getThread} (\todo{ref}) with \texttt{global\_tid}.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_push\_num\_threads}, label={lst:push-num-threads},
                   escapechar=@]
void __kmpc_push_num_threads(ident_t *loc, kmp_int32 global_tid,
                             kmp_int32 num_threads) {
  kmp::runtime::getThread(global_tid).requestNumThreads(num_threads);
};
\end{lstlisting}

\subsection{Work Sharing Constructs}

The following entrypoints are used when running work sharing constructs such as static \texttt{for}
loops or \texttt{sections}. Contrary to GOMP, LLVM uses the same entrypoints for both of them.

\subsubsection{\texttt{\_\_kmpc\_for\_static\_init\_4}}

\paragraph{Description} This function is called by every thread at the beginning of a static work
sharing construct. It is responsible for setting the values of \texttt{plastiter}, \texttt{plower},
\texttt{pupper}, and \texttt{pstride} in order to assign a range of iterations to each thread.
Because this assignment is static, the function only needs to be called once per thread. There is a
variant of this function for handling the case where the loop iteration variable is unsigned. The
semantics and implementation are the same except for the type of \texttt{plower}, \texttt{pupper}
and \texttt{plastiter}, which become unsigned.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{schedtype}: Type of scheduling to be used.
	      \footnote{\url{
			      https://github.com/llvm/llvm-project/blob/
			      f28c006a5895fc0e329fe15fead81e37457cb1d1/openmp/runtime/src/kmp.h\#L357}}
	\item \texttt{plastiter}: Pointer to the \emph{last iteration} flag. This is set to 1 if the
	      calling thread is the one to execute the last iteration of the loop.
	\item \texttt{plower}: Pointer to the lower bound of the iteration range for the current thread.
	\item \texttt{pupper}: Pointer to the upper bound of the iteration range for the current thread.
	\item \texttt{pstride}: Pointer to the stride of the iteration range.
	\item \texttt{incr}: Increment amount of the loop iteration variable.
	\item \texttt{chunk}: Chunk size.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{forStaticInit} method (\todo{ref})
on the object representing the current team.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_for\_static\_init\_4}, label={lst:for-static-init-4},
                   escapechar=@]
void __kmpc_for_static_init_4(ident_t *loc, kmp_int32 gtid,
                              kmp_int32 schedtype,
                              kmp_int32 *plastiter, kmp_int32 *plower,
                              kmp_int32 *pupper, kmp_int32 *pstride,
                              kmp_int32 incr, kmp_int32 chunk) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->forStaticInit(
      loc, gtid, static_cast<kmp_sched_type>(schedtype), plastiter,
      plower, pupper, pstride, incr, chunk);
};
\end{lstlisting}

\subsubsection{\texttt{\_\_kmpc\_for\_static\_fini}}

\paragraph{Description} This function is called when the static work sharing construct is finished.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} This function is not required for the correct implementation of the our
runtime so it does nothing.\footnote{Because the code generated from the OpenMP directives still
	calls this function, it is necessary to implement it even if it is left empty.}

\begin{lstlisting}[language=C, caption={\_\_kmpc\_for\_static\_fini}, label={lst:for-static-fini},
                   escapechar=@]
void __kmpc_for_static_fini(ident_t *loc, kmp_int32 global_tid){};
\end{lstlisting}

\subsection{Dynamic Loops}

\subsubsection{\texttt{__kmpc_dispatch_init_4}}

\paragraph{Description} This function is called once by each thread before running a dynamic
\texttt{for} loop. It is responsible for storing the loop iteration variables and scheduling type so
that they can be used by future calls to \texttt{__kmpc_dispatch_next_4}
(\cref{subsubsec:kmpc-dispatch-next-4}). Just like for the static loop, there is also a variant of
this function that handles unsigned loop iteration variables.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{schedtype}: Type of scheduling to be used.
	      \footnote{\url{
			      https://github.com/llvm/llvm-project/blob/
			      f28c006a5895fc0e329fe15fead81e37457cb1d1/openmp/runtime/src/kmp.h\#L357}}
	\item \texttt{lower}: Lower bound of the iteration range for the current thread.
	\item \texttt{upper}: Upper bound of the iteration range for the current thread.
	\item \texttt{incr}: Increment amount of the loop iteration variable.
	\item \texttt{chunk}: Chunk size.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{dispatchInit} method (\todo{ref}) on
the object representing the current team after removing any scheduling modifiers from the schedule
variable using the \texttt{SCHEDULE\_WITHOUT\_MODIFIERS} macro since they are not supported by our
runtime.

\begin{lstlisting}[language=C, caption={__kmpc_dispatch_init_4},
                   label={lst:kmpc-dispatch-init-4}, escapechar=@]
void __kmpc_dispatch_init_4(ident_t *loc, kmp_int32 gtid,
                            kmp_int32 schedtype, kmp_int32 lower,
                            kmp_int32 upper, kmp_int32 incr,
                            kmp_int32 chunk) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->dispatchInit(
    loc, gtid,
    static_cast<kmp_sched_type>(SCHEDULE_WITHOUT_MODIFIERS(schedtype)),
    lower, upper, incr, chunk);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_dispatch_next_4}}
\label{subsubsec:kmpc-dispatch-next-4}

\paragraph{Description} This function is called by each thread after calling \texttt{dispatchInit}
and after each iteration of the loop. It is responsible for setting the iteration variables for the
current iteration of the loop that the calling thread should execute as well as updating the loop
information shared across the team so that future calling threads can know what work is left to do.
There is also a variant of this function that handles unsigned loop iteration variables.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{plastiter}: Pointer to the \emph{last iteration} flag. This is set to 1 if the
	      calling thread is the one to execute the last iteration of the loop.
	\item \texttt{plower}: Pointer to the lower bound of the iteration range for the current thread.
	\item \texttt{pupper}: Pointer to the upper bound of the iteration range for the current thread.
	\item \texttt{pstride}: Pointer to the stride of the iteration range.
\end{itemize}

\paragraph{Return Value} 1 if there is still work to do for the calling thread, 0 otherwise.

\paragraph{Implementation} This function just calls the \texttt{dispatchNext} method (\todo{ref}) on
the object representing the current team.

\begin{lstlisting}[language=C, caption={__kmpc_dispatch_next_4},
                   label={lst:kmpc-dispatch-next-4}, escapechar=@]
int __kmpc_dispatch_next_4(ident_t *loc, kmp_int32 gtid,
                           kmp_int32 *plastiter, kmp_int32 *plower,
                           kmp_int32 *pupper, kmp_int32 *pstride) {
  return static_cast<int>(
      kmp::runtime::getThread(gtid).getCurrentTeam()->dispatchNext(
          loc, gtid, plastiter, plower, pupper, pstride));
}
\end{lstlisting}

\subsection{Critical Sections}

\subsubsection{\texttt{__kmpc_critical and __kmpc_end_critical}}

\paragraph{Description} These functions are used to implement critical sections.\\
\texttt{\_\_kmpc\_critical} is called before entering a critical section and
\texttt{\_\_kmpc\_end\_critical} is called before leaving it.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{crit}: Pointer to a region of memory associated with the critical section. 8
	      $\times$ 32 bits are made available by the compiler at the location pointed to by this
	      argument.
\end{itemize}

\paragraph{Implementation} Both of these functions reinterpret the memory location pointed to by
\texttt{crit} as a \texttt{kmp::Mutex} (\todo{ref}) object and use it to lock and unlock access to
the critical section. We use a static assertion to make sure that the size of the
\texttt{kmp::Mutex} object is smaller than the size of the memory region.

\begin{lstlisting}[language=C, caption={__kmpc_critical and __kmpc_end_critical},
                   label={lst:kmpc-critical}, escapechar=@]
typedef kmp_int32 kmp_critical_name[8];

void __kmpc_critical(ident_t *loc, kmp_int32 gtid,
                     kmp_critical_name *crit) {
  static_assert(sizeof(kmp::Mutex) <= sizeof(kmp_critical_name));

  kmp::Mutex *mutex = reinterpret_cast<kmp::Mutex *>(*crit);
  mutex->lock();
};

void __kmpc_end_critical(ident_t *loc, kmp_int32 gtid,
                         kmp_critical_name *crit) {
		Mutex *mutex = reinterpret_cast<kmp::Mutex *>(*crit);
		mutex->unlock();
};
\end{lstlisting}

\subsection{Master and Single Constructs}

\subsubsection{\texttt{__kmpc_master and __kmpc_single}}

\paragraph{Description} These functions are used to implement master and single constructs
respectively. They are called by every thread before entering a master or single section.
\texttt{\_\_kmpc\_master} needs to make sure that only the master thread of the team executes the
section, while \texttt{\_\_kmpc\_single} needs to make sure that only a single thread executes it,
however, it does not need to be the master thread.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Return Value} 1 if the calling thread should execute the section, 0 otherwise.

\paragraph{Implementation} Both \texttt{\_\_kmpc\_master} and \texttt{\_\_kmpc\_single} are
implemented identically for simplicity, since only letting the master thread execute single sections
will always be correct, as there is only a single master thread per team. It essentially obtains the
thread ID of the calling thread by calling \texttt{getTid()} on the thread object and compares it
against 0, returning 1 if they are equal and 0 otherwise.

\begin{lstlisting}[language=C, caption={__kmpc_master and __kmpc_single},
                   label={lst:kmpc-master}, escapechar=@]
kmp_int32 __kmpc_master(ident_t *loc, kmp_int32 gtid) {
  return static_cast<kmp_int32>(
    kmp::runtime::getThread(gtid).getTid() == 0
  );
};

kmp_int32 __kmpc_single(ident_t *loc, kmp_int32 gtid) {
  return static_cast<kmp_int32>(
    kmp::runtime::getThread(gtid).getTid() == 0
  );
};
\end{lstlisting}

\subsubsection{\texttt{__kmpc_end_master and __kmpc_end_single}}

\paragraph{Description} These functions are called after the execution of a master or single section
respectively by the thread that executed it.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} These functions are not required for the correct implementation of our
runtime so they do nothing.\footnote{Because the code generated from the OpenMP directives still
	calls these functions, it is necessary to implement them even if they are left empty.}

\begin{lstlisting}[language=C, caption={__kmpc_end_master and __kmpc_end_single},
                   label={lst:kmpc-end-master}, escapechar=@]
void __kmpc_end_master(ident_t *loc, kmp_int32 gtid){};

void __kmpc_end_single(ident_t *loc, kmp_int32 gtid){};
\end{lstlisting}

\subsection{Copyprivate Clause}

\subsubsection{\texttt{\_\_kmpc\_copyprivate}}

\paragraph{Description} This function is used to implement the \texttt{copyprivate} clause
associated with a single section. It is called by every thread that participates in the parallel
section after the single section is executed and it is responsible for broadcasting the private data
of the thread executing the single section to the other threads.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{cpy\_size}: Size of the private data to be copied.
	\item \texttt{cpy\_data}: Pointer to the private data to be copied.
	\item \texttt{cpy\_func}: Pointer to a function that copies the private data.
	\item \texttt{didit}: 1 if the calling thread executed the single section, 0 otherwise.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{copyPrivate} method (\todo{ref}) on
the object representing the current thread.

\begin{lstlisting}[language=C, caption={__kmpc_copyprivate},
                   label={lst:kmpc-copyprivate}, escapechar=@]
void __kmpc_copyprivate(ident_t *loc, kmp_int32 gtid, size_t cpy_size,
                        void *cpy_data,
                        void (*cpy_func)(void *, void *),
                        kmp_int32 didit) {
  kmp::runtime::getThread(gtid).copyPrivate(loc, gtid, cpy_size,
                                            cpy_data, cpy_func, didit);
};
\end{lstlisting}

\subsection{Reduction Clause}

\subsubsection{\texttt{__kmpc_reduce and __kmpc_reduce_nowait}}

\paragraph{Description} These functions are used to implement the reduction clause. The former
performs a barrier after the reduction, while the latter does not.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global_tid}: Global ID of the calling thread.
	\item \texttt{num\_vars}: Number of variables to reduce.
	\item \texttt{reduce\_size}: Size of the data to be reduced (in bytes).
	\item \texttt{reduce\_data}: Pointer to the data to be reduced.
	\item \texttt{reduce\_func}: Pointer to the function that performs the reduction on a pair of
	      elements. The result is made available at the location pointed to by the first argument
	      after calling it.
	\item \texttt{lck}: Pointer to a region of memory associated with the critical section used for
	      the reduction. 8 $\times$ 32 bits are made available by the compiler at the location pointed
	      to by this argument.
\end{itemize}

\paragraph{Return Value} 1 for the master thread, 0 for others. 2 if the reduction should be
performed using build in atomic operations.

\paragraph{Implementation} Both functions are implemented identically since they only differ in the
execution of a barrier, which is differentiated when calling \texttt{\_\_kmpc\_end\_reduce\_nowait}
and \texttt{\_\_kmpc\_end\_reduce} (\cref{subsubsec:kmpc-end-reduce}). They always return 2 in order
to use built-in atomic operations for the reduction.

\begin{lstlisting}[language=C, caption={__kmpc_reduce and __kmpc_reduce_nowait},
                   label={lst:kmpc-reduce}, escapechar=@]
kmp_int32 __kmpc_reduce_nowait(ident_t *loc, kmp_int32 global_tid,
                               kmp_int32 num_vars, size_t reduce_size,
                               void *reduce_data,
                               void (* reduce_func)(void *lhs_data,
                                                    void *rhs_data),
                               kmp_critical_name *lck) {
  return 2; // Atomic reduction
}

kmp_int32 __kmpc_reduce(ident_t *loc, kmp_int32 global_tid,
                        kmp_int32 num_vars, size_t reduce_size,
                        void *reduce_data,
                        void (*reduce_func)(void *lhs_data,
                                            void *rhs_data),
                        kmp_critical_name *lck) {
  return __kmpc_reduce_nowait(loc, global_tid, num_vars, reduce_size,
                              reduce_data, reduce_func, lck);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_end_reduce and __kmpc_end_reduce_nowait}}
\label{subsubsec:kmpc-end-reduce}

\paragraph{Description} These functions are called after the reduction is done. The former performs
a barrier, while the latter does not.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global_tid}: Global ID of the calling thread.
	\item \texttt{lck}: Pointer to a region of memory associated with the critical section used for
	      the reduction. 8 $\times$ 32 bits are made available by the compiler at the location pointed
	      to by this argument.
\end{itemize}

\paragraph{Implementation} \texttt{\_\_kmpc\_end\_reduce\_nowait} does nothing\footnote{Because the
	code generated from the OpenMP directives still calls this function, it is necessary to implement it
	even if it is left empty.}, while \texttt{\_\_kmpc\_end\_reduce} performs a barrier using
\texttt{\_\_kmpc\_barrier} (\cref{subsubsec:kmpc-barrier}).

\begin{lstlisting}[language=C, caption={__kmpc_end_reduce and __kmpc_end_reduce_nowait},
                   label={lst:kmpc-end-reduce}, escapechar=@]
void __kmpc_end_reduce_nowait(ident_t *loc, kmp_int32 global_tid,
                              kmp_critical_name *lck) {}

void __kmpc_end_reduce(ident_t *loc, kmp_int32 global_tid,
                       kmp_critical_name * lck) {
  return __kmpc_barrier(loc, global_tid);
}
\end{lstlisting}

\subsection{Teams Construct}

\subsubsection{\texttt{__kmpc_fork_teams}}
\label{subsubsec:kmpc-fork-teams}

\paragraph{Description} This function is used to implement the \texttt{teams} construct. It is
responsible for creating a league of teams as described in \cref{sec:openmp}.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{argc}: Number of arguments passed to the microtask function.
	\item \texttt{microtask}: Function pointer to the task to be run by each thread in the team.
	\item \texttt{...}: Variable number of arguments to be passed to the microtask function.
\end{itemize}

\paragraph{Implementation} This function works very similarly to \texttt{__kmpc_fork_call}
(\cref{subsubsec:kmpc-fork-call}) except that it calls the \texttt{forkTeams} method (\todo{ref})
and that the microtask will only be executed by the master thread of each team.

\begin{lstlisting}[language=C, caption={__kmpc_fork_teams}, label={lst:kmpc-fork-teams}, escapechar=@]
void __kmpc_fork_teams(ident_t *loc, kmp_int32 argc,
                       kmpc_micro microtask, ...) {
  va_list args;
  va_start(args, microtask);
  kmp::Task kmpMicrotask(microtask, reinterpret_cast<void **>(args),
                         argc);
  kmp::runtime::getCurrentThread().forkTeams(kmpMicrotask);
  va_end(args);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_push_num_teams}}
\label{subsubsec:kmpc-push-num-teams}

\paragraph{Description} This function is used to implement both the \texttt{num_teams} and
\texttt{thread_limit} clauses of the teams construct. \texttt{num\_teams} requests the number of
teams to create and \texttt{thread\_limit} limits the number of threads in each team.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
	\item \texttt{num\_teams}: Requested number of teams.
	\item \texttt{num\_threads}: Requested maximum number of threads per team.
\end{itemize}

\paragraph{Implementation} This function first checks if either \texttt{num\_teams} or
\texttt{num\_threads} are greater than 0 and, if so, it sets the corresponding global variables
(\todo{ref}) accordingly.

\begin{lstlisting}[language=C, caption={__kmpc_push_num_teams}, label={lst:kmpc-push-num-teams}, escapechar=@]
void __kmpc_push_num_teams(ident_t *loc, kmp_int32 global_tid,
                           kmp_int32 num_teams,
                           kmp_int32 num_threads) {
  if (num_teams > 0) {
    kmp::runtime::requestedNumTeams = num_teams;
  }

  if (num_threads > 0) {
    kmp::runtime::requestedThreadLimit = num_threads;
  }
}
\end{lstlisting}

\subsection{Barrier}

\subsubsection{\texttt{__kmpc_barrier}}
\label{subsubsec:kmpc-barrier}

\paragraph{Description} This function is called to execute a barrier.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
      \item \texttt{global\_tid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} This function obtains the barrier object associated with the current team
by calling the \texttt{getBarrier} method (\todo{ref}) and executes it by calling \texttt{wait}.

\begin{lstlisting}[language=C, caption={__kmpc_barrier}, label={lst:kmpc-barrier}, escapechar=@]
void __kmpc_barrier(ident_t *loc, kmp_int32 global_tid) {
  kmp::runtime::getThread(global_tid).getCurrentTeam()
                                        ->getBarrier().wait();
};
\end{lstlisting}

\subsection{Miscellaneous}

\subsubsection{\texttt{__kmpc_global_thread_num}}
\label{subsubsec:kmpc-global-thread-num}

\paragraph{Description} This function is used to obtain the global ID of the calling thread.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
\end{itemize}

\paragraph{Return Value} Global ID of the calling thread.

\paragraph{Implementation} This function just returns the value returned by
\texttt{mempool_get_core_id}, which is provided by the MemPool runtime library. It is used to obtain
the current core ID, which is the same as the global thread ID since each thread runs on a single
core with the same ID.

\begin{lstlisting}[language=C, caption={__kmpc_global_thread_num}, label={lst:kmpc-global-thread-num},
                   escapechar=@]
kmp_int32 __kmpc_global_thread_num(ident_t *loc) {
  return static_cast<kmp_int32>(mempool_get_core_id());
};
\end{lstlisting}

\section{C++ Classes}
\label{sec:cpp-classes}

\section{Supporting Code}
\label{sec:supporting-code}

{\color{red}
	This chapter explains your contributions, be it algorithms, architectures, libraries, hardware, or others.
	As usual, follow a top-down approach and break the chapter into meaningful sections.
	It may even be necessary to split the chapter into multiple chapters (e.g., to separate architecture from implementation in a hardware design).

	Derive a structure for this chapter that is meaningful for your report and discuss it with your advisors.
}
