\chapter{Implementation}
\label{ch:implementation}

\section{General Architecture}
\label{sec:general-architecture}

The OpenMP runtime library presented in this report is implemented in C++ and is structured into
the following major components:

\begin{itemize}
	\item KMP entrypoints.
	\item C++ classes for each relevant concept (e.g., threads, teams, barriers, etc.).
	\item Supporting code that bridges the gap between higher level concepts and the underlying
	      MemPool architecture.
\end{itemize}

Each of them will be described in more detail in the following sections.

\section{KMP Entrypoints}
\label{sec:kmp-entrypoints}

The KMP entrypoints are the functions that are called at runtime when running an OpenMP program.
During the compilation process, the compiler converts OpenMP directives into calls to these
functions as defined by the internal \gls{api} between the LLVM compiler and the runtime library. An
LLVM OpenMP runtime reference document is available at~\cite{kmpref}, however it is quite out of date
and not very helpful in documenting the expected behavior of the runtime library. For this reason,
using the source code of the default runtime
library\footnote{\url{https://github.com/llvm/llvm-project/tree/main/openmp}}, as well as smaller
third-party implementations\footnote{\url{https://github.com/parallel-runtimes/lomp}} can be quite
helpful.

In the following, we will go trough each of the implemented entrypoints grouped by the OpenMP
construct or clause that requires them.

\subsection{Parallel Construct}
\label{subsec:parallel-construct}

\subsubsection{\texttt{\_\_kmpc\_fork\_call}}
\label{subsubsec:kmpc-fork-call}

\paragraph{Description} This function is called by the master thread of the current team (the
default team contains all cores and the core with ID 0 runs the master thread). It is responsible
for assigning the appropriate number of threads to the team and waking them up, as well as setting
up the task that they will run.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{argc}: Number of arguments passed to the microtask function.
	\item \texttt{microtask}: Function pointer to the task to be run by each thread in the team.
	\item \texttt{...}: Variable number of arguments to be passed to the microtask function.
\end{itemize}

\paragraph{Implementation} First, it creates a \texttt{kmp::Task} (\todo{ref}) object with the
microtask, arguments casted to a void pointer array, and the number of such arguments. Then, it
obtains the object representing the current thread using \texttt{kmp::runtime::getCurrentThread()}
(\todo{ref}) and calls the \texttt{forkCall} (\todo{ref}) method on it with the task as an argument.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_fork\_call}, label={lst:fork-call},
                   escapechar=@]
void __kmpc_fork_call(ident_t *loc, kmp_int32 argc,
                      kmpc_micro microtask, ...) {
  va_list args;
  va_start(args, microtask);
  kmp::Task kmpMicrotask(microtask, reinterpret_cast<void **>(args),
                         argc);
  kmp::runtime::getCurrentThread().forkCall(kmpMicrotask);
  va_end(args);
};
\end{lstlisting}

\subsubsection{\texttt{\_\_kmpc\_push\_num\_threads}}
\label{subsubsec:kmpc-push-num-threads}

\paragraph{Description} This function is called when using the \texttt{num\_threads} clause in a
parallel construct. It is used to request a specific number of threads to be used in the parallel
section.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
	\item \texttt{num\_threads}: Requested number of threads.
\end{itemize}

\paragraph{Implementation} This function essentially just calls the \texttt{requestNumThreads}
method (\todo{ref}) on the object representing the current thread obtained by calling
\texttt{kmp::runtime::getThread} (\todo{ref}) with \texttt{global\_tid}.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_push\_num\_threads}, label={lst:push-num-threads},
                   escapechar=@]
void __kmpc_push_num_threads(ident_t *loc, kmp_int32 global_tid,
                             kmp_int32 num_threads) {
  kmp::runtime::getThread(global_tid).requestNumThreads(num_threads);
};
\end{lstlisting}

\subsection{Work Sharing Constructs}

The following entrypoints are used when running work sharing constructs such as static \texttt{for}
loops or \texttt{sections}. Contrary to GOMP, LLVM uses the same entrypoints for both of them.

\subsubsection{\texttt{\_\_kmpc\_for\_static\_init\_4}}

\paragraph{Description} This function is called by every thread at the beginning of a static work
sharing construct. It is responsible for setting the values of \texttt{plastiter}, \texttt{plower},
\texttt{pupper}, and \texttt{pstride} in order to assign a range of iterations to each thread.
Because this assignment is static, the function only needs to be called once per thread. There is a
variant of this function for handling the case where the loop iteration variable is unsigned. The
semantics and implementation are the same except for the type of \texttt{plower}, \texttt{pupper}
and \texttt{plastiter}, which becomes unsigned.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{schedtype}: Type of scheduling to be used.
	      \footnote{\url{
			      https://github.com/llvm/llvm-project/blob/%
			      f28c006a5895fc0e329fe15fead81e37457cb1d1/openmp/runtime/src/kmp.h\#L357}}
	\item \texttt{plastiter}: Pointer to the \emph{last iteration} flag. This is set to 1 if the
	      calling thread is the one to execute the last iteration of the loop.
	\item \texttt{plower}: Pointer to the lower bound of the iteration range for the current thread.
	\item \texttt{pupper}: Pointer to the upper bound of the iteration range for the current thread.
	\item \texttt{pstride}: Pointer to the stride of the iteration range.
	\item \texttt{incr}: Increment amount of the loop iteration variable.
	\item \texttt{chunk}: Chunk size.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{forStaticInit} method (\todo{ref})
on the object representing the current team.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_for\_static\_init\_4}, label={lst:for-static-init-4},
                   escapechar=@]
void __kmpc_for_static_init_4(ident_t *loc, kmp_int32 gtid,
                              kmp_int32 schedtype,
                              kmp_int32 *plastiter, kmp_int32 *plower,
                              kmp_int32 *pupper, kmp_int32 *pstride,
                              kmp_int32 incr, kmp_int32 chunk) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->forStaticInit(
      loc, gtid, static_cast<kmp_sched_type>(schedtype), plastiter,
      plower, pupper, pstride, incr, chunk);
};
\end{lstlisting}

\subsubsection{\texttt{\_\_kmpc\_for\_static\_fini}}

\paragraph{Description} This function is called when the static work sharing construct is finished.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} This function is not required for the correct implementation of the our
runtime so it does nothing.\footnote{Because the code generated from the OpenMP directives still
	calls this function, it is necessary to implement it even if it is left empty.}

\begin{lstlisting}[language=C, caption={\_\_kmpc\_for\_static\_fini}, label={lst:for-static-fini},
                   escapechar=@]
void __kmpc_for_static_fini(ident_t *loc, kmp_int32 global_tid){};
\end{lstlisting}

\subsection{Dynamic Loops}

\subsubsection{\texttt{__kmpc_dispatch_init_4}}

\paragraph{Description} This function is called once by each thread before running a dynamic
\texttt{for} loop. It is responsible for storing the loop iteration variables and scheduling type so
that they can be used by future calls to \texttt{__kmpc_dispatch_next_4}
(\cref{subsubsec:kmpc-dispatch-next-4}). Just like for the static loop, there is also a variant of
this function that handles unsigned loop iteration variables.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{schedtype}: Type of scheduling to be used.
	      \footnote{\url{
			      https://github.com/llvm/llvm-project/blob/%
			      f28c006a5895fc0e329fe15fead81e37457cb1d1/openmp/runtime/src/kmp.h\#L357}}
	\item \texttt{lower}: Lower bound of the iteration range for the current thread.
	\item \texttt{upper}: Upper bound of the iteration range for the current thread.
	\item \texttt{incr}: Increment amount of the loop iteration variable.
	\item \texttt{chunk}: Chunk size.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{dispatchInit} method (\todo{ref}) on
the object representing the current team after removing any scheduling modifiers from the schedule
variable using the \texttt{SCHEDULE\_WITHOUT\_MODIFIERS} macro since they are not supported by our
runtime.

\begin{lstlisting}[language=C, caption={__kmpc_dispatch_init_4},
                   label={lst:kmpc-dispatch-init-4}, escapechar=@]
void __kmpc_dispatch_init_4(ident_t *loc, kmp_int32 gtid,
                            kmp_int32 schedtype, kmp_int32 lower,
                            kmp_int32 upper, kmp_int32 incr,
                            kmp_int32 chunk) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->dispatchInit(
    loc, gtid,
    static_cast<kmp_sched_type>(SCHEDULE_WITHOUT_MODIFIERS(schedtype)),
    lower, upper, incr, chunk);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_dispatch_next_4}}
\label{subsubsec:kmpc-dispatch-next-4}

\paragraph{Description} This function is called by each thread after calling \texttt{dispatchInit}
and after completing the previously assigned chunk of iterations. It is responsible for setting the
iteration variables for the current chunk of the loop that the calling thread should execute, as
well as updating the loop information shared across the team so that future calling threads can know
what work is left to do. There is also a variant of this function that handles unsigned loop
iteration variables.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{plastiter}: Pointer to the \emph{last iteration} flag. This is set to 1 if the
	      calling thread is the one to execute the last iteration of the loop.
	\item \texttt{plower}: Pointer to the lower bound of the iteration range for the current thread.
	\item \texttt{pupper}: Pointer to the upper bound of the iteration range for the current thread.
	\item \texttt{pstride}: Pointer to the stride of the iteration range.
\end{itemize}

\paragraph{Return Value} 1 if there is still work to do for the calling thread, 0 otherwise.

\paragraph{Implementation} This function just calls the \texttt{dispatchNext} method (\todo{ref}) on
the object representing the current team.

\begin{lstlisting}[language=C, caption={__kmpc_dispatch_next_4},
                   label={lst:kmpc-dispatch-next-4}, escapechar=@]
int __kmpc_dispatch_next_4(ident_t *loc, kmp_int32 gtid,
                           kmp_int32 *plastiter, kmp_int32 *plower,
                           kmp_int32 *pupper, kmp_int32 *pstride) {
  return static_cast<int>(
      kmp::runtime::getThread(gtid).getCurrentTeam()->dispatchNext(
          loc, gtid, plastiter, plower, pupper, pstride));
}
\end{lstlisting}

\subsection{\texttt{critical} sections}

\subsubsection{\texttt{__kmpc_critical and __kmpc_end_critical}}

\paragraph{Description} These functions are used to implement \texttt{critical} sections.\\
\texttt{\_\_kmpc\_critical} is called before entering a \texttt{critical} section and
\texttt{\_\_kmpc\_end\_critical} is called before leaving it.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{crit}: Pointer to a region of memory associated with the \texttt{critical} section. 8
	      $\times$ 32 bits are made available by the compiler at the location pointed to by this
	      argument.
\end{itemize}

\paragraph{Implementation} Both of these functions reinterpret the memory location pointed to by
\texttt{crit} as a \texttt{kmp::Mutex} (\todo{ref}) object and use it to lock and unlock access to
the \texttt{critical} section. We use a static assertion to make sure that the size of the
\texttt{kmp::Mutex} object is smaller than the size of the memory region.

\begin{lstlisting}[language=C, caption={__kmpc_critical and __kmpc_end_critical},
                   label={lst:kmpc-critical}, escapechar=@]
typedef kmp_int32 kmp_critical_name[8];

void __kmpc_critical(ident_t *loc, kmp_int32 gtid,
                     kmp_critical_name *crit) {
  static_assert(sizeof(kmp::Mutex) <= sizeof(kmp_critical_name));

  kmp::Mutex *mutex = reinterpret_cast<kmp::Mutex *>(*crit);
  mutex->lock();
};

void __kmpc_end_critical(ident_t *loc, kmp_int32 gtid,
                         kmp_critical_name *crit) {
  Mutex *mutex = reinterpret_cast<kmp::Mutex *>(*crit);
  mutex->unlock();
};
\end{lstlisting}

\subsection{Master and Single Constructs}

\subsubsection{\texttt{__kmpc_master and __kmpc_single}}

\paragraph{Description} These functions are used to implement \texttt{master} and \texttt{single}
constructs respectively. They are called by every thread before entering a \texttt{master} or
\texttt{single} section. \texttt{\_\_kmpc\_master} needs to make sure that only the master thread of
the team executes the section, while \texttt{\_\_kmpc\_single} needs to make sure that only a single
thread executes it, however, it does not need to be the master thread.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Return Value} 1 if the calling thread should execute the section, 0 otherwise.

\paragraph{Implementation} Both \texttt{\_\_kmpc\_master} and \texttt{\_\_kmpc\_single} are
implemented identically for simplicity, since only letting the master thread execute \texttt{single}
sections will always be correct, as there is only a single master thread per team. It essentially
obtains the thread ID of the calling thread by calling \texttt{getTid()} on the thread object and
compares it against 0, returning 1 if they are equal and 0 otherwise.

\begin{lstlisting}[language=C, caption={__kmpc_master and __kmpc_single},
                   label={lst:kmpc-master}, escapechar=@]
kmp_int32 __kmpc_master(ident_t *loc, kmp_int32 gtid) {
  return static_cast<kmp_int32>(
    kmp::runtime::getThread(gtid).getTid() == 0
  );
};

kmp_int32 __kmpc_single(ident_t *loc, kmp_int32 gtid) {
  return static_cast<kmp_int32>(
    kmp::runtime::getThread(gtid).getTid() == 0
  );
};
\end{lstlisting}

\subsubsection{\texttt{__kmpc_end_master and __kmpc_end_single}}

\paragraph{Description} These functions are called after the execution of a \texttt{master} or
\texttt{single} section respectively by the thread that executed it.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} These functions are not required for the correct implementation of our
runtime so they do nothing.\footnote{Because the code generated from the OpenMP directives still
	calls these functions, it is necessary to implement them even if they are left empty.}

\begin{lstlisting}[language=C, caption={__kmpc_end_master and __kmpc_end_single},
                   label={lst:kmpc-end-master}, escapechar=@]
void __kmpc_end_master(ident_t *loc, kmp_int32 gtid){};

void __kmpc_end_single(ident_t *loc, kmp_int32 gtid){};
\end{lstlisting}

\subsection{Copyprivate Clause}

\subsubsection{\texttt{\_\_kmpc\_copyprivate}}

\paragraph{Description} This function is used to implement the \texttt{copyprivate} clause
associated with a \texttt{single} section. It is called by every thread that participates in the
\texttt{parallel} section after the \texttt{single} section is executed and it is responsible for
broadcasting the private data of the thread that executed it to the other threads.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{cpy\_size}: Size of the private data to be copied.
	\item \texttt{cpy\_data}: Pointer to the private data to be copied.
	\item \texttt{cpy\_func}: Pointer to a function that copies the private data.
	\item \texttt{didit}: 1 if the calling thread executed the single section, 0 otherwise.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{copyPrivate} method (\todo{ref}) on
the object representing the current thread.

\begin{lstlisting}[language=C, caption={__kmpc_copyprivate},
                   label={lst:kmpc-copyprivate}, escapechar=@]
void __kmpc_copyprivate(ident_t *loc, kmp_int32 gtid, size_t cpy_size,
                        void *cpy_data,
                        void (*cpy_func)(void *, void *),
                        kmp_int32 didit) {
  kmp::runtime::getThread(gtid).copyPrivate(loc, gtid, cpy_size,
                                            cpy_data, cpy_func, didit);
};
\end{lstlisting}

\subsection{Reduction Clause}

\subsubsection{\texttt{__kmpc_reduce and __kmpc_reduce_nowait}}

\paragraph{Description} These functions are used to implement the reduction clause. The
\texttt{nowait} version is called when the \texttt{nowait} clause is present in the reduction
directive and indicates that no barrier should be executed after the reduction.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global_tid}: Global ID of the calling thread.
	\item \texttt{num\_vars}: Number of variables to reduce.
	\item \texttt{reduce\_size}: Size of the data to be reduced (in bytes).
	\item \texttt{reduce\_data}: Pointer to the data to be reduced.
	\item \texttt{reduce\_func}: Pointer to the function that performs the reduction on a pair of
	      elements. The result is made available at the location pointed to by the first argument
	      after calling it.
	\item \texttt{lck}: Pointer to a region of memory associated with the \texttt{critical} section used for
	      the reduction. 8 $\times$ 32 bits are made available by the compiler at the location pointed
	      to by this argument.
\end{itemize}

\paragraph{Return Value} 1 for the master thread, 0 for others. 2 if the reduction should be
performed using build in atomic operations.

\paragraph{Implementation} Both functions are implemented identically since they only differ in the
execution of a barrier, which is differentiated when calling \texttt{\_\_kmpc\_end\_reduce\_nowait}
and \texttt{\_\_kmpc\_end\_reduce} (\cref{subsubsec:kmpc-end-reduce}). They always return 2 in order
to use built-in atomic operations for the reduction.

\begin{lstlisting}[language=C, caption={__kmpc_reduce and __kmpc_reduce_nowait},
                   label={lst:kmpc-reduce}, escapechar=@]
kmp_int32 __kmpc_reduce_nowait(ident_t *loc, kmp_int32 global_tid,
                               kmp_int32 num_vars, size_t reduce_size,
                               void *reduce_data,
                               void (* reduce_func)(void *lhs_data,
                                                    void *rhs_data),
                               kmp_critical_name *lck) {
  return 2; // Atomic reduction
}

kmp_int32 __kmpc_reduce(ident_t *loc, kmp_int32 global_tid,
                        kmp_int32 num_vars, size_t reduce_size,
                        void *reduce_data,
                        void (*reduce_func)(void *lhs_data,
                                            void *rhs_data),
                        kmp_critical_name *lck) {
  return __kmpc_reduce_nowait(loc, global_tid, num_vars, reduce_size,
                              reduce_data, reduce_func, lck);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_end_reduce and __kmpc_end_reduce_nowait}}
\label{subsubsec:kmpc-end-reduce}

\paragraph{Description} These functions are called after the reduction is done. The first one
performs a barrier, while the second one does not.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global_tid}: Global ID of the calling thread.
	\item \texttt{lck}: Pointer to a region of memory associated with the \texttt{critical} section
	      used for the reduction. 8 $\times$ 32 bits are made available by the compiler at the
	      location pointed to by this argument.
\end{itemize}

\paragraph{Implementation} \texttt{\_\_kmpc\_end\_reduce\_nowait} does nothing\footnote{Because the
	code generated from the OpenMP directives still calls this function, it is necessary to implement it
	even if it is left empty.}, while \texttt{\_\_kmpc\_end\_reduce} performs a barrier using
\texttt{\_\_kmpc\_barrier} (\cref{subsubsec:kmpc-barrier}).

\begin{lstlisting}[language=C, caption={__kmpc_end_reduce and __kmpc_end_reduce_nowait},
                   label={lst:kmpc-end-reduce}, escapechar=@]
void __kmpc_end_reduce_nowait(ident_t *loc, kmp_int32 global_tid,
                              kmp_critical_name *lck) {}

void __kmpc_end_reduce(ident_t *loc, kmp_int32 global_tid,
                       kmp_critical_name * lck) {
  return __kmpc_barrier(loc, global_tid);
}
\end{lstlisting}

\subsection{Teams Construct}

\subsubsection{\texttt{__kmpc_fork_teams}}
\label{subsubsec:kmpc-fork-teams}

\paragraph{Description} This function is used to implement the \texttt{teams} construct. It is
responsible for creating a league of teams as described in \cref{sec:openmp}.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{argc}: Number of arguments passed to the microtask function.
	\item \texttt{microtask}: Function pointer to the task to be run by each thread in the team.
	\item \texttt{...}: Variable number of arguments to be passed to the microtask function.
\end{itemize}

\paragraph{Implementation} This function works very similarly to \texttt{__kmpc_fork_call}
(\cref{subsubsec:kmpc-fork-call}) except that it calls the \texttt{forkTeams} method (\todo{ref})
and that the microtask will only be executed by the master thread of each team.

\begin{lstlisting}[language=C, caption={__kmpc_fork_teams}, label={lst:kmpc-fork-teams}, escapechar=@]
void __kmpc_fork_teams(ident_t *loc, kmp_int32 argc,
                       kmpc_micro microtask, ...) {
  va_list args;
  va_start(args, microtask);
  kmp::Task kmpMicrotask(microtask, reinterpret_cast<void **>(args),
                         argc);
  kmp::runtime::getCurrentThread().forkTeams(kmpMicrotask);
  va_end(args);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_push_num_teams}}
\label{subsubsec:kmpc-push-num-teams}

\paragraph{Description} This function is used to implement both the \texttt{num_teams} and
\texttt{thread_limit} clauses of the teams construct. \texttt{num\_teams} requests the number of
teams to create and \texttt{thread\_limit} limits the number of threads in each team.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
	\item \texttt{num\_teams}: Requested number of teams.
	\item \texttt{num\_threads}: Requested maximum number of threads per team.
\end{itemize}

\paragraph{Implementation} This function first checks if either \texttt{num\_teams} or
\texttt{num\_threads} are greater than 0 and, if so, it sets the corresponding global variables
(\todo{ref}) accordingly. The maximum number of teams is limited to half the number of cores.

\begin{lstlisting}[language=C, caption={__kmpc_push_num_teams}, label={lst:kmpc-push-num-teams}, escapechar=@]
void __kmpc_push_num_teams(ident_t *loc, kmp_int32 global_tid,
                           kmp_int32 num_teams,
                           kmp_int32 num_threads) {
  if (num_teams > 0) {
    kmp::runtime::requestedNumTeams = std::min(num_teams, NUM_CORES / 2);
  }

  if (num_threads > 0) {
    kmp::runtime::requestedThreadLimit = num_threads;
  }
}
\end{lstlisting}

\subsection{Barrier}

\subsubsection{\texttt{__kmpc_barrier}}
\label{subsubsec:kmpc-barrier}

\paragraph{Description} This function is called to execute a barrier.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} This function obtains the barrier object associated with the current team
by calling the \texttt{getBarrier} method (\todo{ref}) and executes it by calling \texttt{wait}.

\begin{lstlisting}[language=C, caption={__kmpc_barrier}, label={lst:kmpc-barrier}, escapechar=@]
void __kmpc_barrier(ident_t *loc, kmp_int32 global_tid) {
  kmp::runtime::getThread(global_tid).getCurrentTeam()
                                        ->getBarrier().wait();
};
\end{lstlisting}

\subsection{Miscellaneous}

\subsubsection{\texttt{__kmpc_global_thread_num}}
\label{subsubsec:kmpc-global-thread-num}

\paragraph{Description} This function is used to obtain the global ID of the calling thread.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
\end{itemize}

\paragraph{Return Value} Global ID of the calling thread.

\paragraph{Implementation} This function just returns the value returned by
\texttt{mempool_get_core_id}, which is provided by the MemPool runtime library. It is used to obtain
the current core ID, which is the same as the global thread ID since each thread runs on a single
core with the same ID.

\begin{lstlisting}[language=C, caption={__kmpc_global_thread_num}, label={lst:kmpc-global-thread-num},
                   escapechar=@]
kmp_int32 __kmpc_global_thread_num(ident_t *loc) {
  return static_cast<kmp_int32>(mempool_get_core_id());
};
\end{lstlisting}

\section{C++ Classes}
\label{sec:cpp-classes}

\subsection{Task}

The \texttt{Task} class is essentially a wrapper around a KMP \texttt{microtask}. This microtask
represents a section of code to be run by a thread (e.g., the body of a \texttt{parallel} section).
Additionally, this class also stores the arguments that must be passed to the \texttt{microtask}.
These are used by the compiler to access variables outside of the \texttt{microtask}'s scope, which
is why they are \texttt{void} pointers.

\subsubsection{Attributes}

\begin{itemize}
	\item \texttt{kmpc_micro func}: Pointer to the \texttt{microtask} function.
	\item \texttt{kmp_int32 argc}: Number of arguments passed to the \texttt{microtask} function.
	\item \texttt{void **args}: Array containing the \texttt{microtask} arguments.
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void run(kmp_int32 gtid, kmp_int32 tid)}: Calls the \texttt{microtask} function
	      with the arguments stored in \texttt{args} in addition to \texttt{gtid} and \texttt{tid},
	      which are always expected to be passed as the first and second argument respectively.
\end{itemize}

\subsection{Barrier}
This class is used to implement a thread barrier.

\subsubsection{Attributes}
\begin{itemize}
	\item \texttt{std::atomic<kmp_int32> barrier}: Atomic counter used to keep track of the number of
	      threads that have reached the barrier.
	\item \texttt{std::atomic<kmp_int32> generation}: Atomic counter used to distinguish between
	      generations when using the \emph{generation barrier}. This is not used in the \emph{WFI
		      barrier}.
	\item \texttt{volatile int32_t numThreads}: Number of threads participating in the barrier.
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void wait()}: This function implements the actual barrier. It distinguishes
	      between two cases: if there is currently only a single team running, then a
	      \emph{\gls{wfi} barrier} is used, otherwise a spinning \emph{generation barrier} is used.

	      The first kind is essentially the same as in \cref{subsec:barriers}, with the only
	      difference being that the \texttt{barrier} variable is not global, but a member of the
	      \texttt{Barrier} class. This is important for the second kind of barrier, but it can also
	      be used in this case for simplicity.

	      If there are multiple teams, then a \emph{generation barrier} is used. This is because the
	      \emph{WFI barrier} does not work when multiple teams are running concurrently, since the
	      call to \texttt{wake\_up\_all} would wake up all threads in all teams, which would
	      interfere with other barriers. This barrier works similarly, but instead of waiting for an
	      interrupt, the threads spin on the \texttt{generation} variable, which is incremented by
	      the last arriving thread.

	      \begin{lstlisting}[language=C, caption={Barrier::wait}, label={lst:barrier-wait},
          escapechar=@]
            inline void wait() {
              if (runtime::numTeams == 1) {
                // WFI barrier

                // Increment the barrier counter
                if ((numThreads - 1) == barrier.fetch_add(1, std::memory_order_relaxed)) {
                  barrier.store(0, std::memory_order_relaxed);
                  std::atomic_thread_fence(std::memory_order_seq_cst);
                  wake_up_all();
                }

                // Some threads have not reached the barrier --> Let's wait
                // Clear the wake-up trigger for the last core reaching the barrier as
                // well
                mempool_wfi();

              } else {
                // Spin generation barrier
                kmp_int32 gen = generation;

                // Increment the barrier counter
                if ((numThreads - 1) == barrier.fetch_add(1, std::memory_order_relaxed)) {
                  barrier.store(0, std::memory_order_relaxed);
                  generation.fetch_add(1, std::memory_order_relaxed);
                  std::atomic_thread_fence(std::memory_order_seq_cst);
                }

                while (gen == generation.load(std::memory_order_relaxed)) {
                  // Spin
                }
              }
            };
          \end{lstlisting}
\end{itemize}

\subsection{Thread}

This class represents a single OpenMP thread running on a core.

\subsubsection{Attributes}

\begin{itemize}
	\item \texttt{kmp_int32 gtid}: Global ID of the thread.
	\item \texttt{kmp_int32 tid}: Team-local ID of the thread.
	\item \texttt{std::optional<Task> teamsRegion}: Optional task representing the body of the
	      \texttt{teams} construct. This is only present in the master thread of the team.
	\item \texttt{std::atomic<Team *> currentTeam}: Pointer to the team the thread is part of.
	\item \texttt{Mutex running}: Mutex indicating whether the thread is running. It is locked if it
	      is, unlocked otherwise. \todo{Explain why this is necessary.}
	\item \texttt{std::optional<kmp_int32> requestedNumThreads}: Optional number of threads requested
	      by a previous call to \texttt{__kmpc_push_num_threads} (\cref{subsubsec:kmpc-push-num-threads}).
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void Thread::run()}: This function implements the infinite loop that each thread
	      except the thread with global ID 0 runs to wait for and execute work assigned to it.

	      First, the thread waits for an interrupt. After it is woken up, it acquires the \texttt{running}
	      mutex and checks that it is part of a team by ensuring that \texttt{currentTeam} is not
	      \texttt{null}, and whether it is the master thread of a team by checking if
	      \texttt{teamsRegion} has a value.

	      If \texttt{currentTeam} is not \texttt{null} and \texttt{teamsRegion} is empty, meaning
	      that the calling thread is a worker thread, the thread executes the implicit task of the
	      team (\todo{ref}), resets the \texttt{currentTeam} pointer, and executes the team's
	      barrier associated with the end of the \texttt{parallel} region. The team pointer has to
	      be reset before executing the barrier to avoid the case where a new team is assigned to
	      the thread after the barrier but before resetting the pointer, which would cause the
	      thread to miss the assignment to the new team.

	      If \texttt{teamsRegion} has a value, that means that the current thread is the master
	      thread of a team and therefore has to execute the body of the \texttt{teams} construct.
	      After doing that, it resets the \texttt{teamsRegion} pointer, deletes the team object, and
	      executes the runtime's barrier associated with the end of the \texttt{teams} construct.
	      (\todo{ref}).

	      \begin{lstlisting}[language=C, caption={Thread::run}, label={lst:thread-run},
          escapechar=@]
            void Thread::run() {
              while (true) {
                mempool_wfi();
                std::lock_guard<Mutex> lock(running);

                if (currentTeam != nullptr && !teamsRegion.has_value()) {

                  (*currentTeam).getImplicitTask()->run(gtid, tid);

                  Team *prevTeam = currentTeam;
                  currentTeam = nullptr;

                  (*prevTeam).getBarrier().wait();

                } else if (teamsRegion.has_value()) {
                  teamsRegion->run(gtid, tid);

                  teamsRegion.reset();

                  delete currentTeam;
                  currentTeam = nullptr;

                  runtime::teamsBarrier.wait();
                }
              }
            };
          \end{lstlisting}

	\item \texttt{void Thread::forkCall(Task parallelRegion)}: This function is called by the master
	      thread of a team when it encouters a \texttt{parallel} construct.

	      It first checks if a specific number of threads has been requested and uses the total
	      number of cores as the default if not. Then, it sets the number of threads for the team as
	      well as the implicit task, which is then run on all threads in the team including the
	      calling thread. Finally, it executes the team's barrier.

	      \begin{lstlisting}[language=C, caption={Thread::forkCall},
          label={lst:thread-forkCall},
          escapechar=@]
            void Thread::forkCall(Task parallelRegion) {
              kmp_int32 numThreads = this->requestedNumThreads
                                            .value_or(NUM_CORES);
              this->requestedNumThreads.reset();

              Team *team = currentTeam;

              // Setup
              team->setNumThreads(numThreads);
              team->setImplicitTask(parallelRegion);

              // Run on all threads
              team->run();
              parallelRegion.run(gtid, tid);

              team->getBarrier().wait();
            };
    \end{lstlisting}

	\item \texttt{void Thread::forkTeams(Task teamsRegion)}: This function is called by the thread
	      with global ID 0 when it encounters a \texttt{teams} construct.

	      First, it checks if a specific number of teams was requested and uses the number of groups
	      (\cref{subsec:mempool_architecture}) as the default if not. This hints at the possibility
	      of having different MemPool groups work on different parts of a given problem or on
	      different problems altogether in the future. Then it sets both the global variable
	      \texttt{runtime::numTeams} (\todo{ref}) and the number of threads participating in the
	      global \texttt{runtime::teamsBarrier} (\todo{ref}) accordingly, and resets
	      \texttt{runtime::requestedNumTeams} for the next call.

          Next, it identifies the master threads of each team by dividing the total number of cores
          into \texttt{runtime::numTeams} equally sized chunks. Then, for each thread except for the
          calling thread\footnote{The thread with global ID 0 is always part of the default team,
          therefore it is not necessary to create a new team for it.}(\todo{ref in footnote}), a new
          team is created with master thread \texttt{coreId} and team ID \texttt{i}. After setting
          the \texttt{teams} region and the requested thread limit, if any, the thread is woken up.

	      The calling thread also sets its \texttt{teams} region and explicitly runs it after
	      setting the requested thread limit if applicable. After executing the region, it is reset
	      and the global \texttt{teams} barrier is executed, finally resetting the number of teams
	      to 1.

	      \begin{lstlisting}[language=C, caption={void Thread::forkTeams},
          label={lst:thread-forkTeams},
          escapechar=@]
            void Thread::forkTeams(Task teamsRegion) {
              runtime::numTeams = runtime::requestedNumTeams
                                            .value_or(NUM_GROUPS);
              runtime::teamsBarrier.setNumThreads(runtime::numTeams);
              runtime::requestedNumTeams.reset();

              kmp_int32 coresPerTeam = NUM_CORES / runtime::numTeams;

              for (kmp_int32 i = 1; i < runtime::numTeams; i++) {
                kmp_int32 coreId = i * coresPerTeam;

                Thread &thread = runtime::getThread(coreId);

                thread.setCurrentTeam(new Team(coreId, i));
                thread.setTeamsRegion(teamsRegion);

                if (runtime::requestedThreadLimit) {
                  thread.requestNumThreads(
                          runtime::requestedThreadLimit.value());
                }

                thread.wakeUp();
              }

              this->setTeamsRegion(teamsRegion);
              if (runtime::requestedThreadLimit) {
                this->requestNumThreads(
                        runtime::requestedThreadLimit.value());
              }

              teamsRegion.run(gtid, tid);
              this->teamsRegion.reset();

              runtime::teamsBarrier.wait();

              runtime::numTeams = 1;
            };
    \end{lstlisting}

\end{itemize}

\subsection{Team}

\section{Supporting Code}
\label{sec:supporting-code}

\subsection{Runtime Namespace}

\subsection{Heap Allocation}

\subsection{Main Function Wrapper}
