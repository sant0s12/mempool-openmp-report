\chapter{Implementation Details}

\section{KMP Entrypoints}
\label{sec:kmp-entrypoints}

\subsection{Parallel Construct}
\label{subsec:parallel-construct}

\subsubsection{\texttt{\_\_kmpc\_fork\_call}}
\label{subsubsec:kmpc-fork-call}

\paragraph{Description} This function is called by the master thread of the current team (the
default team contains all cores and the core with ID 0 runs the master thread). It is responsible
for assigning the appropriate number of threads to the team and waking them up, as well as setting
up the task that they will run.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{argc}: Number of arguments passed to the microtask function.
	\item \texttt{microtask}: Function pointer to the task to be run by each thread in the team.
	\item \texttt{...}: Variable number of arguments to be passed to the microtask function.
\end{itemize}

\paragraph{Implementation} First, it creates a \texttt{kmp::Task} (\cref{subsec:task}) object with
the microtask, arguments casted to a void pointer array, and the number of such arguments. Then, it
obtains the object representing the current thread using \texttt{kmp::runtime::getCurrentThread()}
(\cref{subsec:runtime-namespace}) and calls the \texttt{forkCall} method on it with the task as an
argument.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_fork\_call}, label={lst:fork-call},
                   escapechar=@]
void __kmpc_fork_call(ident_t *loc, kmp_int32 argc,
                      kmpc_micro microtask, ...) {
  va_list args;
  va_start(args, microtask);
  kmp::Task kmpMicrotask(microtask, reinterpret_cast<void **>(args),
                         argc);
  kmp::runtime::getCurrentThread().forkCall(kmpMicrotask);
  va_end(args);
};
\end{lstlisting}

\subsubsection{\texttt{\_\_kmpc\_push\_num\_threads}}
\label{subsubsec:kmpc-push-num-threads}

\paragraph{Description} This function is called when using the \texttt{num\_threads} clause in a
parallel construct. It is used to request a specific number of threads to be used in the parallel
section.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
	\item \texttt{num\_threads}: Requested number of threads.
\end{itemize}

\paragraph{Implementation} This function essentially just calls the \texttt{requestNumThreads}
method on the object representing the current thread (\cref{subsec:thread}), which sets the
\texttt{requestedNumThreads} field, obtained by calling \texttt{kmp::runtime::getThread}
(\cref{subsec:runtime-namespace}) with \texttt{global\_tid}.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_push\_num\_threads}, label={lst:push-num-threads},
                   escapechar=@]
void __kmpc_push_num_threads(ident_t *loc, kmp_int32 global_tid,
                             kmp_int32 num_threads) {
  kmp::runtime::getThread(global_tid).requestNumThreads(num_threads);
};
\end{lstlisting}

\subsection{Work Sharing Constructs}

The following entrypoints are used when running work sharing constructs such as static \texttt{for}
loops or \texttt{sections}. Contrary to GOMP, LLVM uses the same entrypoints for both of them.

\subsubsection{\texttt{\_\_kmpc\_for\_static\_init\_4}}

\paragraph{Description} This function is called by every thread at the beginning of a static work
sharing construct. It is responsible for setting the values of \texttt{plastiter}, \texttt{plower},
\texttt{pupper}, and \texttt{pstride} in order to assign a range of iterations to each thread.
Because this assignment is static, the function only needs to be called once per thread. There is a
variant of this function for handling the case where the loop iteration variable is unsigned. The
semantics and implementation are the same except for the type of \texttt{plower}, \texttt{pupper}
and \texttt{plastiter}, which becomes unsigned.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{schedtype}: Type of scheduling to be used.
	      \footnote{\url{
			      https://github.com/llvm/llvm-project/blob/%
			      f28c006a5895fc0e329fe15fead81e37457cb1d1/openmp/runtime/src/kmp.h\#L357}}
	\item \texttt{plastiter}: Pointer to the \emph{last iteration} flag. This is supposed to be set
	      to 1 if the calling thread is the one to execute the last iteration of the loop.
	\item \texttt{plower}: Pointer to the lower bound of the iteration range for the current thread.
	      This is initially set to the global lower bound for the loop.
	\item \texttt{pupper}: Pointer to the upper bound of the iteration range for the current thread.
	      This is initially set to the global lower bound for the loop.
	\item \texttt{pstride}: Pointer to the stride of the iteration range. This is the distance between
	      two work chunks assigned to the same thread.
	\item \texttt{incr}: Increment amount of the loop iteration variable.
	\item \texttt{chunk}: Chunk size.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{forStaticInit} method
(\cref{subsubsec:team-forstaticinit}) on the object representing the current team.

\begin{lstlisting}[language=C, caption={\_\_kmpc\_for\_static\_init\_4}, label={lst:for-static-init-4},
                   escapechar=@]
void __kmpc_for_static_init_4(ident_t *loc, kmp_int32 gtid,
                              kmp_int32 schedtype,
                              kmp_int32 *plastiter, kmp_int32 *plower,
                              kmp_int32 *pupper, kmp_int32 *pstride,
                              kmp_int32 incr, kmp_int32 chunk) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->forStaticInit(
      loc, gtid, static_cast<kmp_sched_type>(schedtype), plastiter,
      plower, pupper, pstride, incr, chunk);
};
\end{lstlisting}

\subsubsection{\texttt{\_\_kmpc\_for\_static\_fini}}

\paragraph{Description} This function is called when the static work sharing construct is finished.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} This function is not required for the correct implementation of the our
runtime so it does nothing.\footnote{Because the code generated from the OpenMP directives still
	calls this function, it is necessary to implement it even if it is left empty.}

\begin{lstlisting}[language=C, caption={\_\_kmpc\_for\_static\_fini}, label={lst:for-static-fini},
                   escapechar=@]
void __kmpc_for_static_fini(ident_t *loc, kmp_int32 global_tid){};
\end{lstlisting}

\subsection{Dynamic Loops}

\subsubsection{\texttt{__kmpc_dispatch_init_4}}

\paragraph{Description} This function is called once by each thread before running a dynamic
\texttt{for} loop. It is responsible for storing the loop iteration variables and scheduling type so
that they can be used by future calls to \texttt{__kmpc_dispatch_next_4}
(\cref{subsubsec:kmpc-dispatch-next-4}). Just like for the static loop, there is also a variant of
this function that handles unsigned loop iteration variables.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{schedtype}: Type of scheduling to be used.
	      \footnote{\url{
			      https://github.com/llvm/llvm-project/blob/%
			      f28c006a5895fc0e329fe15fead81e37457cb1d1/openmp/runtime/src/kmp.h\#L357}}
	\item \texttt{lower}: Lower bound of the iteration range for the current thread.
	\item \texttt{upper}: Upper bound of the iteration range for the current thread.
	\item \texttt{incr}: Increment amount of the loop iteration variable.
	\item \texttt{chunk}: Chunk size.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{dispatchInit} method
(\cref{subsec:team-dispatch-init}) on the object representing the current team after removing any
scheduling modifiers from the schedule variable using the \texttt{SCHEDULE\_WITHOUT\_MODIFIERS}
macro since they are not supported by our runtime.

\begin{lstlisting}[language=C, caption={__kmpc_dispatch_init_4},
                   label={lst:kmpc-dispatch-init-4}, escapechar=@]
void __kmpc_dispatch_init_4(ident_t *loc, kmp_int32 gtid,
                            kmp_int32 schedtype, kmp_int32 lower,
                            kmp_int32 upper, kmp_int32 incr,
                            kmp_int32 chunk) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->dispatchInit(
    loc, gtid,
    static_cast<kmp_sched_type>(SCHEDULE_WITHOUT_MODIFIERS(schedtype)),
    lower, upper, incr, chunk);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_dispatch_next_4}}
\label{subsubsec:kmpc-dispatch-next-4}

\paragraph{Description} This function is called by each thread after calling \texttt{dispatchInit}
and after completing the previously assigned chunk of iterations. It is responsible for setting the
iteration variables for the current chunk of the loop that the calling thread should execute, as
well as updating the loop information shared across the team so that future calling threads can know
what work is left to do. There is also a variant of this function that handles unsigned loop
iteration variables.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{plastiter}: Pointer to the \emph{last iteration} flag. This is supposed to be set
	      to 1 if the calling thread is the one to execute the last iteration of the loop.
	\item \texttt{plower}: Pointer to the lower bound of the iteration range for the current thread.
	\item \texttt{pupper}: Pointer to the upper bound of the iteration range for the current thread.
	\item \texttt{pstride}: Pointer to the stride of the iteration range.
\end{itemize}

\paragraph{Return Value} 1 if there is still work to do for the calling thread, 0 otherwise.

\paragraph{Implementation} This function just calls the \texttt{dispatchNext} method
(\cref{subsec:team-dispatch-init}) on the object representing the current team.

\begin{lstlisting}[language=C, caption={__kmpc_dispatch_next_4},
                   label={lst:kmpc-dispatch-next-4}, escapechar=@]
int __kmpc_dispatch_next_4(ident_t *loc, kmp_int32 gtid,
                           kmp_int32 *plastiter, kmp_int32 *plower,
                           kmp_int32 *pupper, kmp_int32 *pstride) {
  return static_cast<int>(
      kmp::runtime::getThread(gtid).getCurrentTeam()->dispatchNext(
          loc, gtid, plastiter, plower, pupper, pstride));
}
\end{lstlisting}

\subsection{Critical sections}

\subsubsection{\texttt{__kmpc_critical and __kmpc_end_critical}}

\paragraph{Description} These functions are used to implement \texttt{critical} sections.\\
\texttt{\_\_kmpc\_critical} is called before entering a \texttt{critical} section and
\texttt{\_\_kmpc\_end\_critical} is called before leaving it.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{crit}: Pointer to a region of memory associated with the \texttt{critical} section. 8
	      $\times$ 32 bits are made available by the compiler at the location pointed to by this
	      argument.
\end{itemize}

\paragraph{Implementation} Both of these functions reinterpret the memory location pointed to by
\texttt{crit} as a \texttt{kmp::Mutex} object and use it to lock and unlock access to the
\texttt{critical} section. We use a static assertion to make sure that the size of the
\texttt{kmp::Mutex} object is smaller than the size of the memory region.

\begin{lstlisting}[language=C, caption={__kmpc_critical and __kmpc_end_critical},
                   label={lst:kmpc-critical}, escapechar=@]
typedef kmp_int32 kmp_critical_name[8];

void __kmpc_critical(ident_t *loc, kmp_int32 gtid,
                     kmp_critical_name *crit) {
  static_assert(sizeof(kmp::Mutex) <= sizeof(kmp_critical_name));

  kmp::Mutex *mutex = reinterpret_cast<kmp::Mutex *>(*crit);
  mutex->lock();
};

void __kmpc_end_critical(ident_t *loc, kmp_int32 gtid,
                         kmp_critical_name *crit) {
  Mutex *mutex = reinterpret_cast<kmp::Mutex *>(*crit);
  mutex->unlock();
};
\end{lstlisting}

\subsection{Master and Single Constructs}

\subsubsection{\texttt{__kmpc_master and __kmpc_single}}

\paragraph{Description} These functions are used to implement the \texttt{master} and
\texttt{single} constructs respectively. They are called by every thread before entering a
\texttt{master} or \texttt{single} section. \texttt{\_\_kmpc\_master} needs to make sure that only
the master thread of the team executes the section, while \texttt{\_\_kmpc\_single} needs to make
sure that only a single thread executes it, however, it does not need to be the master thread.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Return Value} 1 if the calling thread should execute the section, 0 otherwise.

\paragraph{Implementation} Both \texttt{\_\_kmpc\_master} and \texttt{\_\_kmpc\_single} are
implemented identically for simplicity, since only letting the master thread execute \texttt{single}
sections will always be correct, as there is only a single master thread per team. It essentially
obtains the thread ID of the calling thread by calling \texttt{getTid()} on the thread object and
compares it against 0, returning 1 if they are equal and 0 otherwise.

\begin{lstlisting}[language=C, caption={__kmpc_master and __kmpc_single},
                   label={lst:kmpc-master}, escapechar=@]
kmp_int32 __kmpc_master(ident_t *loc, kmp_int32 gtid) {
  return static_cast<kmp_int32>(
    kmp::runtime::getThread(gtid).getTid() == 0
  );
};

kmp_int32 __kmpc_single(ident_t *loc, kmp_int32 gtid) {
  return static_cast<kmp_int32>(
    kmp::runtime::getThread(gtid).getTid() == 0
  );
};
\end{lstlisting}

\subsubsection{\texttt{__kmpc_end_master and __kmpc_end_single}}

\paragraph{Description} These functions are called after the execution of a \texttt{master} or
\texttt{single} section respectively by the thread that executed it.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} These functions are not required for the correct implementation of our
runtime so they do nothing.\footnote{Because the code generated from the OpenMP directives still
	calls these functions, it is necessary to implement them even if they are left empty.}

\begin{lstlisting}[language=C, caption={__kmpc_end_master and __kmpc_end_single},
                   label={lst:kmpc-end-master}, escapechar=@]
void __kmpc_end_master(ident_t *loc, kmp_int32 gtid){};

void __kmpc_end_single(ident_t *loc, kmp_int32 gtid){};
\end{lstlisting}

\subsection{Copyprivate Clause}

\subsubsection{\texttt{\_\_kmpc\_copyprivate}}

\paragraph{Description} This function is used to implement the \texttt{copyprivate} clause
associated with a \texttt{single} section. It is called by every thread that participates in the
\texttt{parallel} section after the \texttt{single} section is executed and it is responsible for
broadcasting the private data of the thread that executed it to the other threads.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{gtid}: Global ID of the calling thread.
	\item \texttt{cpy\_size}: Size of the private data to be copied.
	\item \texttt{cpy\_data}: Pointer to the private data to be copied.
	\item \texttt{cpy\_func}: Pointer to a function that copies the private data.
	\item \texttt{didit}: 1 if the calling thread executed the single section, 0 otherwise.
\end{itemize}

\paragraph{Implementation} This function just calls the \texttt{copyPrivate} method
(\cref{subsec:team-copyprivate}) on the object representing the current team that the calling thread
belongs to.

\begin{lstlisting}[language=C, caption={__kmpc_copyprivate},
                   label={lst:kmpc-copyprivate}, escapechar=@]
void __kmpc_copyprivate(ident_t *loc, kmp_int32 gtid, size_t cpy_size,
                        void *cpy_data,
                        void (*cpy_func)(void *, void *),
                        kmp_int32 didit) {
  kmp::runtime::getThread(gtid).getCurrentTeam()->copyPrivate(
      loc, gtid, cpy_size, cpy_data, cpy_func, didit);
};
\end{lstlisting}

\subsection{Reduction Clause}

\subsubsection{\texttt{__kmpc_reduce and __kmpc_reduce_nowait}}

\paragraph{Description} These functions are used to implement the reduction clause. The
\texttt{nowait} version is called when the \texttt{nowait} clause is present in the reduction
directive and indicates that no barrier should be executed after the reduction.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global_tid}: Global ID of the calling thread.
	\item \texttt{num\_vars}: Number of variables to reduce.
	\item \texttt{reduce\_size}: Size of the data to be reduced (in bytes).
	\item \texttt{reduce\_data}: Pointer to the data to be reduced.
	\item \texttt{reduce\_func}: Pointer to the function that performs the reduction on a pair of
	      elements. The result is made available at the location pointed to by the first argument
	      after calling it.
	\item \texttt{lck}: Pointer to a region of memory associated with the \texttt{critical} section used for
	      the reduction. 8 $\times$ 32 bits are made available by the compiler at the location pointed
	      to by this argument.
\end{itemize}

\paragraph{Return Value} 1 for the master thread, 0 for others. 2 if the reduction should be
performed using build in atomic operations.

\paragraph{Implementation} Both functions are implemented identically since they only differ in the
execution of a barrier, which is differentiated when calling \texttt{\_\_kmpc\_end\_reduce\_nowait}
and \texttt{\_\_kmpc\_end\_reduce} (\cref{subsubsec:kmpc-end-reduce}). They always return 2 in order
to use built-in atomic operations for the reduction.

\begin{lstlisting}[language=C, caption={__kmpc_reduce and __kmpc_reduce_nowait},
                   label={lst:kmpc-reduce}, escapechar=@]
kmp_int32 __kmpc_reduce_nowait(ident_t *loc, kmp_int32 global_tid,
                               kmp_int32 num_vars, size_t reduce_size,
                               void *reduce_data,
                               void (* reduce_func)(void *lhs_data,
                                                    void *rhs_data),
                               kmp_critical_name *lck) {
  return 2; // Atomic reduction
}

kmp_int32 __kmpc_reduce(ident_t *loc, kmp_int32 global_tid,
                        kmp_int32 num_vars, size_t reduce_size,
                        void *reduce_data,
                        void (*reduce_func)(void *lhs_data,
                                            void *rhs_data),
                        kmp_critical_name *lck) {
  return __kmpc_reduce_nowait(loc, global_tid, num_vars, reduce_size,
                              reduce_data, reduce_func, lck);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_end_reduce and __kmpc_end_reduce_nowait}}
\label{subsubsec:kmpc-end-reduce}

\paragraph{Description} These functions are called after the reduction is done. The first one
performs a barrier, while the second one does not.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global_tid}: Global ID of the calling thread.
	\item \texttt{lck}: Pointer to a region of memory associated with the \texttt{critical} section
	      used for the reduction. 8 $\times$ 32 bits are made available by the compiler at the
	      location pointed to by this argument.
\end{itemize}

\paragraph{Implementation} \texttt{\_\_kmpc\_end\_reduce\_nowait} does nothing\footnote{Because the
	code generated from the OpenMP directives still calls this function, it is necessary to implement it
	even if it is left empty.}, while \texttt{\_\_kmpc\_end\_reduce} performs a barrier using
\texttt{\_\_kmpc\_barrier} (\cref{subsubsec:kmpc-barrier}).

\begin{lstlisting}[language=C, caption={__kmpc_end_reduce and __kmpc_end_reduce_nowait},
                   label={lst:kmpc-end-reduce}, escapechar=@]
void __kmpc_end_reduce_nowait(ident_t *loc, kmp_int32 global_tid,
                              kmp_critical_name *lck) {}

void __kmpc_end_reduce(ident_t *loc, kmp_int32 global_tid,
                       kmp_critical_name * lck) {
  return __kmpc_barrier(loc, global_tid);
}
\end{lstlisting}

\subsection{Teams Construct}

\subsubsection{\texttt{__kmpc_fork_teams}}
\label{subsubsec:kmpc-fork-teams}

\paragraph{Description} This function is used to implement the \texttt{teams} construct. It is
responsible for creating a league of teams as described in \cref{sec:openmp}.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{argc}: Number of arguments passed to the microtask function.
	\item \texttt{microtask}: Function pointer to the task to be run by each thread in the team.
	\item \texttt{...}: Variable number of arguments to be passed to the microtask function.
\end{itemize}

\paragraph{Implementation} This function works very similarly to \texttt{__kmpc_fork_call}
(\cref{subsubsec:kmpc-fork-call}) except that it calls the \texttt{forkTeams} method
(\cref{subsec:thread-forkteams})
and that the microtask will only be executed by the master thread of each team.

\begin{lstlisting}[language=C, caption={__kmpc_fork_teams}, label={lst:kmpc-fork-teams}, escapechar=@]
void __kmpc_fork_teams(ident_t *loc, kmp_int32 argc,
                       kmpc_micro microtask, ...) {
  va_list args;
  va_start(args, microtask);
  kmp::Task kmpMicrotask(microtask, reinterpret_cast<void **>(args),
                         argc);
  kmp::runtime::getCurrentThread().forkTeams(kmpMicrotask);
  va_end(args);
}
\end{lstlisting}

\subsubsection{\texttt{__kmpc_push_num_teams}}
\label{subsubsec:kmpc-push-num-teams}

\paragraph{Description} This function is used to implement both the \texttt{num_teams} and
\texttt{thread_limit} clauses of the teams construct. \texttt{num\_teams} requests the number of
teams to create and \texttt{thread\_limit} limits the number of threads in each team.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
	\item \texttt{num\_teams}: Requested number of teams.
	\item \texttt{num\_threads}: Requested maximum number of threads per team.
\end{itemize}

\paragraph{Implementation} This function first checks if either \texttt{num\_teams} or
\texttt{num\_threads} are greater than 0 and, if so, it sets the corresponding global variables
(\cref{subsec:runtime-namespace}) accordingly. The maximum number of teams is limited to half the
number of cores.

\begin{lstlisting}[language=C, caption={__kmpc_push_num_teams}, label={lst:kmpc-push-num-teams}, escapechar=@]
void __kmpc_push_num_teams(ident_t *loc, kmp_int32 global_tid,
                           kmp_int32 num_teams,
                           kmp_int32 num_threads) {
  if (num_teams > 0) {
    kmp::runtime::requestedNumTeams = std::min(num_teams, NUM_CORES / 2);
  }

  if (num_threads > 0) {
    kmp::runtime::requestedThreadLimit = num_threads;
  }
}
\end{lstlisting}

\subsection{Barrier}

\subsubsection{\texttt{__kmpc_barrier}}
\label{subsubsec:kmpc-barrier}

\paragraph{Description} This function is called to execute a barrier.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
	\item \texttt{global\_tid}: Global ID of the calling thread.
\end{itemize}

\paragraph{Implementation} This function obtains the barrier object associated with the current team
by calling the \texttt{getBarrier} method and executes it by calling \texttt{wait}.

\begin{lstlisting}[language=C, caption={__kmpc_barrier}, label={lst:kmpc-barrier}, escapechar=@]
void __kmpc_barrier(ident_t *loc, kmp_int32 global_tid) {
  kmp::runtime::getThread(global_tid).getCurrentTeam()
                                        ->getBarrier().wait();
};
\end{lstlisting}

\subsection{Miscellaneous}

\subsubsection{\texttt{__kmpc_global_thread_num}}
\label{subsubsec:kmpc-global-thread-num}

\paragraph{Description} This function is used to obtain the global ID of the calling thread.

\paragraph{Arguments}
\begin{itemize}
	\item \texttt{loc}: Pointer to a struct containing information about the source code location
	      of the call.
\end{itemize}

\paragraph{Return Value} Global ID of the calling thread.

\paragraph{Implementation} This function just returns the value returned by
\texttt{mempool_get_core_id}, which is provided by the MemPool runtime library. It is used to obtain
the current core ID, which is the same as the global thread ID since each thread runs on a single
core with the same ID.

\begin{lstlisting}[language=C, caption={__kmpc_global_thread_num}, label={lst:kmpc-global-thread-num},
                   escapechar=@]
kmp_int32 __kmpc_global_thread_num(ident_t *loc) {
  return static_cast<kmp_int32>(mempool_get_core_id());
};
\end{lstlisting}

\section{C++ Classes}
\label{sec:cpp-classes}

\subsection{Task}
\label{subsec:task}

\subsubsection{Attributes}

\begin{itemize}
	\item \texttt{kmpc_micro func}: Pointer to the \texttt{microtask} function.
	\item \texttt{kmp_int32 argc}: Number of arguments passed to the \texttt{microtask} function.
	\item \texttt{void **args}: Array containing the \texttt{microtask} arguments.
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void run(kmp_int32 gtid, kmp_int32 tid)}: Calls the \texttt{microtask} function
	      with the arguments stored in \texttt{args} in addition to \texttt{gtid} and \texttt{tid},
	      which are always expected to be passed as the first and second argument respectively.
\end{itemize}

\subsection{Barrier}
\label{subsec:barrier}

\subsubsection{Attributes}
\begin{itemize}
	\item \texttt{std::atomic<kmp_int32> barrier}: Atomic counter used to keep track of the number of
	      threads that have reached the barrier.
	\item \texttt{std::atomic<kmp_int32> generation}: Atomic counter used to distinguish between
	      generations when using the \emph{generation barrier}. This is not used in the \emph{WFI
		      barrier}.
	\item \texttt{int32_t numThreads}: Number of threads participating in the barrier.
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void wait()}: This function implements the actual barrier. It distinguishes
	      between two cases: if there is currently only a single team running, then a
	      \emph{\gls{wfi} barrier} is used, otherwise a spinning \emph{generation barrier} is used.

	      The first kind is essentially the same as in \cref{subsec:barriers}, with the only
	      difference being that the \texttt{barrier} variable is not global, but a member of the
	      \texttt{Barrier} class. This is important for the second kind of barrier, but it can also
	      be used in this case for simplicity.

	      If there are multiple teams, then a \emph{generation barrier} is used. This is because the
	      \emph{WFI barrier} does not work when multiple teams are running concurrently, since the
	      call to \texttt{wake\_up\_all} would wake up all threads in all teams, which would
	      interfere with other barriers. This barrier works similarly, but instead of waiting for an
	      interrupt, the threads spin on the \texttt{generation} variable, which is incremented by
	      the last arriving thread.

	      \begin{lstlisting}[language=C, caption={Barrier::wait}, label={lst:barrier-wait},
          escapechar=@]
            inline void wait() {
              if (runtime::numTeams == 1) {
                // WFI barrier

                // Increment the barrier counter
                if ((numThreads - 1) == barrier.fetch_add(1,
                                                std::memory_order_relaxed)) {
                  barrier.store(0, std::memory_order_relaxed);
                  std::atomic_thread_fence(std::memory_order_seq_cst);
                  wake_up_all();
                }

                // Some threads have not reached the barrier --> Let's wait
                // Clear the wake-up trigger for the last core reaching
                // the barrier as well
                mempool_wfi();

              } else {
                // Spin generation barrier
                kmp_int32 gen = generation;

                // Increment the barrier counter
                if ((numThreads - 1) == barrier.fetch_add(1,
                                                std::memory_order_relaxed)) {
                  barrier.store(0, std::memory_order_relaxed);
                  generation.fetch_add(1, std::memory_order_relaxed);
                  std::atomic_thread_fence(std::memory_order_seq_cst);
                }

                while (gen == generation.load(std::memory_order_relaxed)) {
                  // Spin
                }
              }
            };
          \end{lstlisting}
\end{itemize}

\subsection{Thread}
\label{subsec:thread}

\subsubsection{Attributes}

\begin{itemize}
	\item \texttt{kmp_int32 gtid}: Global ID of the thread.
	\item \texttt{kmp_int32 tid}: Team-local ID of the thread.
	\item \texttt{std::optional<Task> teamsRegion}: Optional task representing the body of the
	      \texttt{teams} construct. This is only present in the master thread of the team.
	\item \texttt{Team *currentTeam}: Pointer to the team the thread is part of.
	\item \texttt{Mutex running}: Mutex indicating whether the thread is running. It is locked if it
	      is, unlocked otherwise. \footnote{This is only necessary to support the
		      Banshee~\cite{banshee} simulator, which does not currently support the accumulation of
		      interrupts and can cause threads to miss their wake-up signal.}
	\item \texttt{std::optional<kmp_int32> requestedNumThreads}: Optional number of threads requested
	      by a previous call to \texttt{__kmpc_push_num_threads} (\cref{subsubsec:kmpc-push-num-threads}).
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void Thread::run()}: This function implements the infinite loop that each thread
	      except the thread with global ID 0 runs to wait for and execute work assigned to it.

	      First, the thread waits for an interrupt. After it is woken up, it acquires the \texttt{running}
	      mutex and checks that it is part of a team by ensuring that \texttt{currentTeam} is not
	      \texttt{null}, and whether it is the master thread of a team by checking if
	      \texttt{teamsRegion} has a value.

	      If \texttt{currentTeam} is not \texttt{null} and \texttt{teamsRegion} is empty, meaning
	      that the calling thread is a worker thread, the thread executes the implicit task of the
	      team (\cref{subsec:team}), resets the \texttt{currentTeam} pointer, sets its \texttt{tid}
	      to match its \texttt{gtid}, and executes the team's barrier associated with the end of the
	      \texttt{parallel} region. The \texttt{tid} is reset in this way to speed up the forking
	      process when launching only one team later, since then every thread's \texttt{tid} matches
	      its \texttt{gtid}. The team pointer has to be reset before executing the barrier to avoid
	      the case where a new team is assigned to the thread after the barrier but before resetting
	      the pointer, which would cause the thread to miss the assignment to the new team.

	      If \texttt{teamsRegion} has a value, that means that the current thread is the master
	      thread of a team and therefore has to execute the body of the \texttt{teams} construct.
	      After doing that, it resets the \texttt{teamsRegion} pointer, deletes the team object, and
	      executes the runtime's barrier associated with the end of the \texttt{teams} construct.
	      (\cref{sec:openmp}).

	      \begin{lstlisting}[language=C, caption={Thread::run}, label={lst:thread-run},
          escapechar=@]
            void Thread::run() {
              while (true) {
                mempool_wfi();
                std::lock_guard<Mutex> lock(running);

                if (currentTeam != nullptr && !teamsRegion.has_value()) {

                  (*currentTeam).getImplicitTask().run(gtid, tid);

                  Team *prevTeam = currentTeam;
                  currentTeam = nullptr;
                  tid = gtid;

                  (*prevTeam).getBarrier().wait();

                } else if (teamsRegion.has_value()) {
                  teamsRegion->run(gtid, tid);

                  teamsRegion.reset();

                  delete currentTeam;
                  currentTeam = nullptr;
                  tid = gtid;

                  runtime::teamsBarrier.wait();
                }
              }
            };
          \end{lstlisting}

	\item \texttt{void Thread::forkCall(Task parallelRegion)}: This function is called by the master
	      thread of a team when it encouters a \texttt{parallel} construct.

	      It first checks if a specific number of threads has been requested and uses the total
	      number of cores as the default if not. Then, it sets the number of threads for the team as
	      well as the implicit task, which is then run on all threads in the team including the
	      calling thread. Finally, it executes the team's barrier.

	      \begin{lstlisting}[language=C, caption={Thread::forkCall},
          label={lst:thread-forkCall},
          escapechar=@]
            void Thread::forkCall(Task parallelRegion) {
              kmp_int32 numThreads = this->requestedNumThreads
                                            .value_or(NUM_CORES);
              this->requestedNumThreads.reset();

              Team *team = currentTeam;

              // Setup
              team->setNumThreads(numThreads);
              team->setImplicitTask(parallelRegion);

              // Run on all threads
              team->run();
              parallelRegion.run(gtid, tid);

              team->getBarrier().wait();
            };
    \end{lstlisting}

	\item \texttt{void Thread::forkTeams(Task teamsRegion)}: \label{subsec:thread-forkteams} This
	      function is called by the thread with global ID 0 when it encounters a \texttt{teams}
	      construct.

	      First, it checks if a specific number of teams was requested and uses the number of groups
	      (\cref{subsec:mempool_architecture}) as the default if not. This hints at the possibility
	      of having different MemPool groups work on different parts of a given problem or on
	      different problems altogether in the future. Then it sets both the global variable
	      \texttt{runtime::numTeams} (\cref{subsec:runtime-namespace}) and the number of threads
	      participating in the global \texttt{runtime::teamsBarrier}
	      (\cref{subsec:runtime-namespace}) accordingly, and resets
	      \texttt{runtime::requestedNumTeams} for the next call.

	      Next, it identifies the master threads of each team by dividing the total number of cores
	      into \texttt{runtime::numTeams} equally sized chunks. Then, for each thread except for the
	      calling thread\footnote{The thread with global ID 0 is always part of the default team
		      (\cref{subsec:runtime-namespace}), therefore it is not necessary to create a new team
		      for it.}, a new team is created with master thread \texttt{coreId} and team ID \texttt{i}.
	      After setting the \texttt{teams} region and the requested thread limit, if any, the thread
	      is woken up.

	      The calling thread also sets its \texttt{teams} region and explicitly runs it after
	      setting the requested thread limit if applicable. After executing the region, it is reset
	      and the global \texttt{teams} barrier is executed, finally resetting the number of teams
	      to 1.

	      \begin{lstlisting}[language=C, caption={void Thread::forkTeams},
          label={lst:thread-forkTeams},
          escapechar=@]
            void Thread::forkTeams(Task teamsRegion) {
              runtime::numTeams = runtime::requestedNumTeams
                                            .value_or(NUM_GROUPS);
              runtime::teamsBarrier.setNumThreads(runtime::numTeams);
              runtime::requestedNumTeams.reset();

              kmp_int32 coresPerTeam = NUM_CORES / runtime::numTeams;

              for (kmp_int32 i = 1; i < runtime::numTeams; i++) {
                kmp_int32 coreId = i * coresPerTeam;

                Thread &thread = runtime::getThread(coreId);

                thread.setCurrentTeam(new Team(coreId, i));
                thread.setTeamsRegion(teamsRegion);

                if (runtime::requestedThreadLimit) {
                  thread.requestNumThreads(
                          runtime::requestedThreadLimit.value());
                }

                thread.wakeUp();
              }

              this->setTeamsRegion(teamsRegion);
              if (runtime::requestedThreadLimit) {
                this->requestNumThreads(
                        runtime::requestedThreadLimit.value());
              }

              teamsRegion.run(gtid, tid);
              this->teamsRegion.reset();

              runtime::teamsBarrier.wait();

              runtime::numTeams = 1;
            };
    \end{lstlisting}

\end{itemize}

\subsection{Team}
\label{subsec:team}

\subsubsection{Attributes}

\begin{itemize}
	\item \texttt{kmp_int32 masterGtid}: Global ID of the master thread of the team.
	\item \texttt{kmp_int32 teamId}: ID of the team.
	\item \texttt{kmp_int32 numThreads}: Number of threads in the team.
	\item \texttt{Barrier barrier}: Barrier associated with the team.
	\item \texttt{DynamicSchedule dynamicSchedule}: Struct containing information related to dynamic
	      loops.
	\item \texttt{void *copyPrivateData}: Pointer to the \texttt{copyprivate} data to be
	      broadcast to the thereads in the team.
	\item \texttt{Task implicitTask}: Task representing the implicit task of the team.
\end{itemize}

\subsubsection{Relevant Methods}

\begin{itemize}
	\item \texttt{void Team::run()}: This function is used to launch the team and is called by the
	      master thread. It distinguishes between two cases:

	      \begin{enumerate}
              \item If there are multiple teams, it iterates sequentially over the threads with a
                    global ID in the range $[\texttt{masterGtid},\: \texttt{masterGtid} +
                    \texttt{numThreads} - 1]$, sets their team local thread ID, and, if they are not the
		            master thread, sets the team pointer and wakes them up.
		      \item If there is only one team, it iterates over all threads in the team in the same
                    way and sets their team pointer. Unlike in the previous case, now all threads
                    can be woken up at once using \texttt{wake\_up\_all}, which is slightly more efficient.
	      \end{enumerate}


	      \begin{lstlisting}[language=C, caption={void Team::run},
          label={lst:team-run}, escapechar=@]
            inline void run() {
              if (runtime::numTeams > 1) {
                for (kmp_int32 i = masterGtid + 1;
                      i < masterGtid + numThreads; i++) {
                  auto &thread = runtime::getThread(i);
                  thread.setTid(i - masterGtid);

                  if (i != masterGtid) {
                    thread.setCurrentTeam(this);
                    thread.wakeUp();
                  }
                }
              } else {
                for (kmp_int32 i = masterGtid + 1;
                      i < masterGtid + numThreads; i++) {
                  auto &thread = runtime::getThread(i);
                  thread.setCurrentTeam(this);
                }

                wake_up_all();
                mempool_wfi();
              }
            }
          \end{lstlisting}

	\item \texttt{void Team::forStaticInit(ident_t *loc, kmp_int32 gtid,\\kmp_sched_type schedtype,
		      T *plastiter, T *plower, T *pupper,\\SignedT *pstride, SignedT incr, SignedT chunk)}:
	      \label{subsubsec:team-forstaticinit}
	      This function is used to determine the static loop parameters for the \texttt{for} and
	      \texttt{distribute} constructs. It is called once by every participating thread.

	      We assume that the loop increment is always 1, since this is the case for the code
	      generated by LLVM 14.

	      It first distinguishes between the \texttt{static} and \texttt{static_chunked} scheduling
	      types. In the former case, the chunk size is not provided and is therefore calculated as
	      the ceiling of the loop size divided by the number of threads, since the OpenMP standard
	      specifies that the work should be distributed as evenly as possible. The switch case then
	      falls through to the \texttt{static_chunked} case, where we assume that the chunk size is
	      already provided.

	      We define the \texttt{span} like on \citeauthor{herokmp}'s work as the distance between two
	      consecutive iterations of the loop, possibly running on different threads. With this, we
	      can calculate the \texttt{stride}, which just accounts for the number of threads in
	      between to iterations running on the same thread.

	      The \texttt{plower} bound is calculated by shifting the \texttt{span} \texttt{tid} times
	      starting from the global lower bound, and \texttt{pupper} is that plus the \texttt{span}
	      minus the increment. The \texttt{plastiter} flag is set to \texttt{true} if the calling
	      thread is the last one to execute the loop.

	      The scheduling related to the \texttt{distribute} construct is almost the same, except
	      that the number of teams is used instead of the number of threads.

	      \begin{lstlisting}[language=C, caption={void Team::forStaticInit},
          label={lst:team-forstaticinit}, escapechar=@]
            template <typename T,
                      typename SignedT = typename std::make_signed<T>::type,
                      typename UnsignedT =
                                  typename std::make_unsigned<T>::type>
            void forStaticInit(ident_t * loc, kmp_int32 gtid,
                               kmp_sched_type schedtype, T *plastiter,
                               T *plower, T *pupper, SignedT *pstride,
                               SignedT incr, SignedT chunk) const {

              assert(incr == 1 && "Loop increment is not 1");

              switch (schedtype) {
              case kmp_sch_static: {

                // Calculate chunk size
                chunk = static_cast<SignedT>(*pupper - *plower + 1) /
                          numThreads +
                        (static_cast<SignedT>(*pupper - *plower + 1) %
                          numThreads != 0);

                // Fall through to static chunked
              }
              case kmp_sch_static_chunked: {
                assert(incr != 0 && "Loop increment must be non-zero");
                assert(chunk > 0 && "Chunk size is not positive");
                assert((static_cast<T>(chunk) <= *pupper - *plower + 1) &&
                       "Chunk size is greater than loop size");

                kmp_int32 tid = runtime::getThread(gtid).getTid();

                SignedT numChunks =
                    (static_cast<SignedT>(*pupper - *plower) + chunk) / chunk;

                SignedT span = incr * chunk;
                *pstride = span * static_cast<SignedT>(numThreads);
                *plower = *plower + static_cast<T>(tid) *
                            static_cast<T>(span);
                *pupper = *plower + static_cast<T>(span - incr);
                *plastiter = (tid == (numChunks - 1) % numThreads);

                break;
              }

              // Distribute (teams)
              case kmp_distribute_static: {

                // Calculate chunk size
                chunk =
                    static_cast<SignedT>(*pupper - *plower + 1) /
                      runtime::numTeams +
                    (static_cast<SignedT>(*pupper - *plower + 1) %
                      runtime::numTeams != 0);

                // Fall through to static chunked
              }
              case kmp_distribute_static_chunked: {
                assert(incr != 0 && "Loop increment must be non-zero");
                assert(chunk > 0 && "Chunk size is not positive");
                assert((static_cast<T>(chunk) <= *pupper - *plower + 1) &&
                       "Chunk size is greater than loop size");

                SignedT numChunks =
                    (static_cast<SignedT>(*pupper - *plower) + chunk) / chunk;

                SignedT span = incr * chunk;
                *pstride = span * static_cast<SignedT>(runtime::numTeams);
                *plower = *plower + static_cast<T>(teamId) *
                            static_cast<T>(span);
                *pupper = *plower + static_cast<T>(span - incr);
                *plastiter = (teamId == (numChunks - 1) % runtime::numTeams);

                break;
              }
              default: {
                assert(false && "Unsupported scheduling type");
                break;
              }
              }
            }
          \end{lstlisting}

	\item \texttt{void Team::void dispatchInit(ident_t * loc, kmp_int32 gtid,\\kmp_sched_type
		      schedtype, T lower, T upper, SignedT incr,\\SignedT chunk)}: \label{subsec:team-dispatch-init}
	      This function is called once by every thread participating in a dynamic loop to initialize the
	      dynamic schedule struct, which is shown in \cref{lst:team-dynamicschedule}. This  contains all
	      of the loop parameters as well as a \texttt{valid} flag indicating whether the schedule has
	      been initialized, a counter \texttt{numDone} to count how many threads have finished executing
	      the loop, and a mutex to lock access to the struct since it is accessed concurrently by
	      multiple threads.

	      Our implementation of the function currently only supports the
	      \texttt{kmp\_sch\_dynamic\_chunked} scheduling type, which is used by dynamic \texttt{for}
	      loops containing either \texttt{schedule(dynamic, chunk)} or \texttt{schedule(dynamic)}
	      clauses.

	      After locking the schedule struct, it first checks if the schedule has already been
	      initialized by another thread and immediately returns if that is the case, since this only
	      needs to be done once.

	      Otherwise, \texttt{lower}, \texttt{upper} and \texttt{chunk} are directly stored, while
	      the remaining parameters are calculated as in \cref{lst:team-forstaticinit}. Finally, the
	      \texttt{valid} flag is set to \texttt{true}.

	      \begin{lstlisting}[language=C, caption={struct Team::DynamicSchedule},
          label={lst:team-dynamicschedule}, escapechar=@]
            struct DynamicSchedule {
              kmp_uint32 lowerNext = 0;
              kmp_uint32 upper = 0;
              kmp_uint32 chunk = 0; // Chunk size assumed to be positive
              kmp_int32 incr = 0;
              kmp_int32 stride = 0;

              bool valid = false;
              kmp_int32 numDone = 0;

              Mutex mutex;
            };
          \end{lstlisting}

	      \begin{lstlisting}[language=C, caption={void Team::dispatchInit},
          label={lst:team-dispatchinit}, escapechar=@]
            template <typename T,
                      typename SignedT = typename std::make_signed<T>::type,
                      typename UnsignedT =
                                  typename std::make_unsigned<T>::type>
            void dispatchInit(ident_t * loc, kmp_int32 gtid,
                              kmp_sched_type schedtype, T lower, T upper,
                              SignedT incr, SignedT chunk) {

              assert(incr == 1 && "Loop increment is not 1");
              assert(chunk > 0 && "Chunk size is not positive");
              assert((static_cast<T>(chunk) <= upper - lower + 1) &&
                     "Chunk size is greater than loop size");

              switch (schedtype) {
              case kmp_sch_dynamic_chunked: {
                std::lock_guard<Mutex> lock(dynamicSchedule.mutex);

                if (dynamicSchedule.valid) {
                  return;
                }

                SignedT span = incr * chunk;

                dynamicSchedule.lowerNext = static_cast<kmp_uint32>(lower);
                dynamicSchedule.upper = static_cast<kmp_uint32>(upper);
                dynamicSchedule.chunk = static_cast<kmp_uint32>(chunk);
                dynamicSchedule.incr = incr;
                dynamicSchedule.stride = span *
                  static_cast<SignedT>(numThreads);

                dynamicSchedule.valid = true;
                break;
              }
              default: {
                assert(false && "Unsupported scheduling type");
                break;
              }
              };
            }
          \end{lstlisting}

	\item \texttt{bool Team::dispatchNext(ident_t *loc, kmp_int32 gtid,\\SignedT *plastiter, T *plower, T
		      *pupper, SignedT *pstride)}:\\ This function is called by a thread executing a dynamic
	      loop to obtain the next chunk of iterations to execute. It uses the type variable
	      \texttt{T} to account for the fact that there are both signed and unsigned versions of
	      the entrypoint (\cref{subsubsec:kmpc-dispatch-next-4}) associated with this function.

	      First, it locks the dynamic schedule object so that it can be safely modified, since
	      other threads will try to access it concurrently. Then, it asserts that the schedule
	      is initialized by checking that \texttt{valid} is \texttt{true}.

	      Next, it checks if there is any work left to be done by seeing if \texttt{lowerNext}
	      is greater than \texttt{upper}. If that is the case, then that means that there are no
	      more work chunks available, so it increments the number of threads that are done. If
	      all are done, it sets the \texttt{valid} flag to \texttt{false} and resets
	      \texttt{numDone} so that the schedule can be reused. It returns \texttt{false} to
	      indicate that there was no chunk left to execute.

	      If there is work left, it sets the lower bound of the current chunk to \texttt{lowerNext}
	      and increments \texttt{lowerNext} by the chunk size. \texttt{lowerNext} is checked
	      again against \texttt{upper} to know if the current chunk is the last one. If that is
	      the case, the upper bound is set to the global upper bound and \texttt{plastiter} is
	      set to \texttt{true}. Otherwise, the upper bound is set to \texttt{lowerNext - 1} and
	      \texttt{plastiter} is set to \texttt{false}. Finally, the stride is set to the stride
	      of the schedule without modification and \texttt{true} is returned to indicate that
	      there was a chunk to execute.

	      \begin{lstlisting}[language=C, caption={void Team::dispatchNext},
          label={lst:team-dispatchnext}, escapechar=@]
            template <typename T,
                      typename SignedT = typename std::make_signed<T>::type>
            bool dispatchNext(ident_t *loc, kmp_int32 gtid,
                              SignedT *plastiter, T *plower,
                              T *pupper, SignedT *pstride) {

              std::lock_guard<Mutex> lock(dynamicSchedule.mutex);
              assert(dynamicSchedule.valid
                      && "Dynamic schedule is not valid");

              if (dynamicSchedule.lowerNext > dynamicSchedule.upper) {
                if (++dynamicSchedule.numDone == numThreads) {
                  dynamicSchedule.valid = false;
                  dynamicSchedule.numDone = 0;
                }

                return false;
              }

              *plower = static_cast<T>(dynamicSchedule.lowerNext);

              dynamicSchedule.lowerNext += dynamicSchedule.chunk;
              if (dynamicSchedule.lowerNext > dynamicSchedule.upper) {
                *pupper = static_cast<T>(dynamicSchedule.upper);
                *plastiter = true;
              } else {
                *pupper = static_cast<T>(dynamicSchedule.lowerNext - 1);
                *plastiter = false;
              }

              *pstride = dynamicSchedule.stride;

              return true;
            };
          \end{lstlisting}

	\item \texttt{void Team::copyPrivate(ident_t *loc, kmp_int32 gtid, size_t cpy_size, void
		      *cpy_data, void (*cpy_func)(void *, void *), kmp_int32 didit)}:
	      \label{subsec:team-copyprivate} This function is used to
	      implement the \texttt{copyprivate} clause of a  \texttt{single} section.

	      If the calling thread executed the \texttt{single} section, as indicated by \texttt{didit}
	      being unequal to 0, the pointer to the private data is stored in the team's \texttt{copyPrivateData}
	      field. Otherwise, the private data is copied by calling \texttt{cpy\_func}.

	      A barrier is executed in between to ensure that the pointer is set before the data is
	      copied. The one at the end makes sure that all thread have copied the data before
	      continuing.

	      \begin{lstlisting}[language=C, caption={void Team::copyprivate},
          label={lst:team-copyprivate}, escapechar=@]
            inline void copyPrivate(ident_t *loc, kmp_int32 gtid,
                                    size_t cpy_size, void *cpy_data,
                                    void (*cpy_func)(void *, void *),
                                    kmp_int32 didit) {
              if (didit != 0) {
                copyPrivateData = cpy_data;
              }

              barrier.wait();

              if (didit == 0) {
                cpy_func(cpy_data, copyPrivateData);
              }

              barrier.wait();
            };
          \end{lstlisting}

	\item \texttt{Team::$\sim$Team}: This is the team's destructor. In order to avoid threads
	      getting stuck in the barrier associated with the team object by spinning on the
	      \texttt{generation} variable (\cref{subsec:barrier}), which would be deallocated when the team
	      object is destroyed, the destructor waits for all threads in the team to go to sleep before
	      destroying the team.

	      \begin{lstlisting}[language=C, caption={Team::$\sim$Team},
          label={lst:team-destructor}, escapechar=@, literate={~} {$\sim$}{1}]
            inline ~Team() {
              for (kmp_int32 i = masterGtid + 1; i < masterGtid + numThreads;
                    i++) {
                while (runtime::getThread(i).isRunning()) {
                  // Wait for thread to finish
                }
              }
            }
          \end{lstlisting}

\end{itemize}

\section{Supporting Code}
\label{sec:supporting-code}

\subsection{Runtime Namespace}
\label{subsec:runtime-namespace}

The \texttt{kmp::runtime} namespace contains global variables used throughout our implementation as
well as some helper functions.

\subsubsection{Global Variables}

\begin{enumerate}
	\item \texttt{threads}: Array of \texttt{Thread} objects, where each one is initialized with the
	      corresponding core ID.
	\item \texttt{defaultTeam}: Default team used if no \texttt{teams} construct is used.
	\item \texttt{requestedNumTeams}: Number of teams requested by the \texttt{num\_teams} clause
	      of a \texttt{teams} construct.
	\item \texttt{requestedThreadLimit}: Number of threads requested by the \texttt{thread\_limit}
	      clause of a \texttt{teams} construct.
	\item \texttt{numTeams}: Number of teams currently running.
	\item \texttt{teamsBarrier}: Barrier used to synchronize the end of a \texttt{teams} construct.
\end{enumerate}

\begin{lstlisting}[language=C, caption={Global Variables},
	label={lst:global-variables}, escapechar=@, literate={~} {$\sim$}{1}]
template <kmp_int32... Is>
constexpr std::array<Thread, sizeof...(Is)>
sequencetoArray(std::integer_sequence<kmp_int32, Is...> /*unused*/) {
  return {{Is...}};
}

std::array<Thread, NUM_CORES> threads =
    sequencetoArray(
        std::make_integer_sequence<kmp_int32, NUM_CORES>{});

Team defaultTeam(0, 0);

std::optional<kmp_int32> requestedNumTeams;
std::optional<kmp_int32> requestedThreadLimit;
kmp_int32 numTeams = 1;

Barrier teamsBarrier(NUM_GROUPS);
\end{lstlisting}

\subsubsection{Helper Functions}

The helper functions are used to access and run threads more conveniently.

\begin{lstlisting}[language=C, caption={Helper Functions},
	label={lst:helper-functions}, escapechar=@, literate={~} {$\sim$}{1}]
static inline void runThread(kmp_int32 core_id) {
  threads[static_cast<kmp_uint32>(core_id)].run();
};

static inline Thread &getThread(kmp_int32 gtid) {
  return threads[static_cast<kmp_uint32>(gtid)];
};

static inline Thread &getCurrentThread() {
  return threads[mempool_get_core_id()];
};
\end{lstlisting}
